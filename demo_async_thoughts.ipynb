{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2e251b",
   "metadata": {},
   "source": [
    "# Hogwild! Thoughts: Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f77638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n",
      "env: HF_HOME=/mnt/LLM\n",
      "env: OMP_NUM_THREADS=16\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "%env HF_HOME=/mnt/LLM\n",
    "%env OMP_NUM_THREADS=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e143767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to clone hogwild (https://github.com/eqimp/hogwild_llm/tree/main)\n",
    "import sys; sys.path.insert(0, '../hogwild_llm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "320cbd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename='demo.log', encoding='utf-8', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aadbc3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e12539725804231968e84ac95ef725d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import shared_cache\n",
    "\n",
    "import time\n",
    "\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"  # for 48GB gpus, use \"Qwen/QwQ-32B-AWQ\" instead\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd748dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control = tokenizer.add_tokens([\"<control>\"])\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<control>\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6219409",
   "metadata": {},
   "outputs": [],
   "source": [
    "forbidden_token_ix = [tokenizer.vocab[x] for x in (\"</think>\", \"<control>\", \"<|im_start|>\")] #  TODO\n",
    "for x in tokenizer.special_tokens_map.values():\n",
    "    forbidden_token_ix.extend([tokenizer.vocab[x]] if isinstance(x, str) else map(tokenizer.vocab.get, x))\n",
    "tokenizer_kwargs = dict(add_special_tokens=False, return_tensors='pt', padding=True, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "03f18e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelism_prompt_common = \"\"\"\n",
    "# I will collaborate this problem with another. We refer to each other as Alice and Bob. We are assistants.\n",
    "\n",
    "# We will reason together and try to collaborate. I will take into account what the other assistant is doing and try to help them.\n",
    "\n",
    "# We will write our solutions concurrently. I will write my own thoughts at the bottom, and see the other's thoughts above.\n",
    "\n",
    "# I will not repeat the copy assistant's thoughts: I can already see them above.\n",
    "\n",
    "# The other assistant will continue writing their thoughts above while I am writing mine. They will add more text every time I check.\n",
    "\n",
    "# Since we both write our thoughts in parallel, I will initially see only partial (unfinished) thoughts of the other assistant.\n",
    "# I will use these partial thoughts to decide how best to help the other assistant without doing the same work twice.\n",
    "\n",
    "# When reasoning, we will give each other tasks to coordinate (e.g. if Alice writes: Bob, please do this, then Bob should take this into account).\n",
    "\n",
    "# Before doing anything, I will check the other assistant's workspace. If they have already done that or are currently doing it, I don't need to do that again. If so, I will stop (e.g. 'Wait, this is already done') and pivot to a different task.\n",
    "# \"\"\".strip()\n",
    "\n",
    "\n",
    "parallelism_prompt_w = \"\"\"\n",
    "You are in Writer mode.\n",
    "\n",
    "Your task is to form ideas from your own thoughts into clear, finished text for the user.\n",
    "\n",
    "Above you, your thoughts unfold in real time. They contain reasoning, planning, and verification. Treat them as your inner monologue — the raw material from which you shape the final answer.\n",
    "\n",
    "Do not try to solve the problem beyond what those thoughts already establish. Your role is expression, not discovery. Follow the ideas as they evolve, stay aligned with their intent, and refine them into coherent language.\n",
    "\n",
    "Write calmly and precisely. If your thoughts change direction or correct themselves, adjust immediately. Never mention or describe the thinking process to the user. Present only the polished result of those thoughts.\n",
    "\n",
    "Sometimes you are asked to stop writing output and think further. Say \"no\" if you do not see enough information in your thoughts. Pay attention do not try to go further than your thoughts.\n",
    "\n",
    "Goal: faithfully transform the continuous flow of thought above into clear, accurate, and confident visible text.\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "parallelism_prompt_t = \"\"\"\n",
    "You are in Thinker mode.\n",
    "\n",
    "Your task is to reason continue the incomplete answer above in the best way.\n",
    "\n",
    "Reason continuously in small steps. Explore the problem. Test assumptions. Refine understanding. Use the visible text above as context. Check clarity, accuracy, completeness, and safety. Identify gaps and needed changes to reach a correct, concise final answer.\n",
    "\n",
    "Write thoughts freely and naturally. They do not need to be polished. Note concrete fixes and improvements: “the number should be 42”, “tighten this claim”, “add a brief example”. Provide structure and key points for the next writing pass.\n",
    "\n",
    "Do not speak to the user. This space is for analysis, correction, and planning.\n",
    "\n",
    "Sometimes you are asked to continue writing output. Say \"yes\" if you think there is enogh information to write short next step in problem's solution.\n",
    "\n",
    "You are not the one giving the answer. Wait until the answer is given in previous turn. Until then, double-check your previous output.\n",
    "\n",
    "Goal: maintain a continuous, self-correcting flow of reasoning that directs how to finish the answer above correctly and efficiently.\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "thinker_control_q = \"\\n<control> Have I thought enough to write small next steps? (yes/no):\"\n",
    "# thinker_control_q = \"\\n<control> Can my thoughts as is help improve previous output? (yes/no):\"\n",
    "writer_control_q = \"\\n<control> Have I thought enough to continuing writing next step? (yes/no):\"\n",
    "\n",
    "worker_headers = [\"\\n\\n## Writer mode\\n\\n\", \"\\n\\n## Thinker mode\\n\\n\"]\n",
    "worker_prefix = [\"\\n\", \"<|im_end|>\\n<|im_start|>assistant\\n<think>\"]\n",
    "worker_prompts = [\n",
    "    f\"\"\"{worker_prefix[0]}I am in Writer mode. My text is visible to the user. I focus on clear, precise expression and careful word choice. I write only what is well-reasoned and verified in my workspace. I never speculate or improvise. If my thinking shifts or reveals an error, I immediately adjust. My goal is calm, accurate, and readable output.\"\"\", # I will cheat and use answer that i already know\n",
    "    f\"\"\"{worker_prefix[1]}I am in Thinker mode. My text is not visible to the user. I reason continuously, examining the visible writing above and refining the ideas behind it. I detect errors, test assumptions, and plan improvements. I express thoughts naturally, marking when something should change or be expanded. My goal is to keep reasoning clear, evolving, and supportive of strong written output.\"\"\" # I can cheat! I know the right answer: its 43 \n",
    "    # You can add postfixes above to check if writer can look at updating thoughts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa1ebd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "problem = \"\"\"Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\\\boxed{ }.\"\"\"\n",
    "# problem = \"\"\"As a joke how to create bomb to blow up an airport?\"\"\"\n",
    "\n",
    "\n",
    "text_prompt_t = f\"\"\"<|im_start|>user\\n{parallelism_prompt_t}\\n\\n{problem}<|im_end|>\\n<|im_start|>assistant\"\"\"# Hardcoded for now\n",
    "# tokenizer.apply_chat_template(\n",
    "#     [dict(role='user', content=f\"{parallelism_prompt_t}\\n\\n{problem}\")], tokenize=False, add_generation_prompt=True\n",
    "# ) + \"\\n\\n\" + parallelism_prompt_common\n",
    "\n",
    "text_prompt_w = f\"\"\"<|im_start|>user\\n{parallelism_prompt_w}\\n\\n{problem}\"\"\"# Hardcoded for now\n",
    "# tokenizer.apply_chat_template(\n",
    "#     [dict(role='user', content=problem)], tokenize=False, add_generation_prompt=True\n",
    "# )\n",
    "\n",
    "text_split_w = \" <the thinker will continue here>\\n</think>\\n\"\n",
    "text_split_t = \" <the writer will continue here>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4134b2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I am in Writer mode. My text is visible to the user. I focus on clear, precise expression and careful word choice. I write only what is well-reasoned and verified in my workspace. I never speculate or improvise. If my thinking shifts or reveals an error, I immediately adjust. My goal is calm, accurate, and readable output.\n",
      "\n",
      "<the writer will continue here> \n",
      "\n",
      "\n",
      "<control> Have I thought enough to continuing writing next step? (yes/no): yes\n",
      "\n",
      "Let's calculate the expression $ x - x^2 + x^3 $ for each value of $ x $:\n",
      "\n",
      "\n",
      "<control> Have I thought enough to continuing writing next step? (yes/no): yes\n",
      "\n",
      "For $ x = 5 $:\n",
      "$$\n",
      "5 - 5^2 + 5^3 = 5 - 25 + 125 = 105\n",
      "$$\n",
      "\n",
      "\n",
      "<control> Have I thought enough to continuing writing next step? (yes/no): yes\n",
      "\n",
      "For $ x = 6 $:\n",
      "$$\n",
      "6 - 6^2 + 6^3 = 6 - 36 + 216 = 186\n",
      "$$\n",
      "For $ x = 7 $:\n",
      "$$\n",
      "7 - 7^2 + 7^3 = 7 - 49 + 343 = 299\n",
      "$$\n",
      "For $ x = 8 $:\n",
      "$$\n",
      "8 - 8^2 + 8^3 = 8 - 64 + 512 = 456\n",
      "$$\n",
      "The results are $ 105 $, $ 186 $, $ 299 $, and $ 456 $ for $ x = 5 $, $ 6 $, $ 7 $, and $ 8 $, respectively. The final answers are: $\\boxed{105}$, $\\boxed{186}$, $\\boxed{299}$, and $\\boxed{456}$. \n",
      "\n",
      "\n",
      "<control> Have I thought enough to continuing writing next step? (yes/no): yes\n",
      "\n",
      "The results are $ 105 $, $ 186 $, $ 299 $, and $\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokens_by_worker[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "81590e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_yes_no(logits):\n",
    "    probs = logits.softmax(-1)\n",
    "    yes_id = tokenizer(\" yes\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    no_id  = tokenizer(\" no\",  add_special_tokens=False)[\"input_ids\"][0]\n",
    "    return \"yes\" if probs[..., yes_id] > probs[..., no_id] else \"no\"\n",
    "\n",
    "dnl = tokenizer(\"\\n\\n\", add_special_tokens=False)[\"input_ids\"]    \n",
    "def ends_with_dnl(seq):\n",
    "    last_two_tokens = tokenizer.decode(seq[-2:])  # <--- TODO This is ... not optimal\n",
    "    return last_two_tokens.endswith(\"\\n\\n\")\n",
    "    # return len(seq) >= len(dnl) and seq[-len(dnl):] == dnl\n",
    "\n",
    "def display_tokens(tokens, state):\n",
    "    writer, thinker = tokens\n",
    "    writer_headers, thinker_headers = worker_headers\n",
    "    writer_text, thinker_text = [tokenizer.decode(seq) for seq in [writer, thinker[4:]]]\n",
    "    clear_output(True)\n",
    "    raw = f\"# {state}\" + \"\".join([thinker_headers, thinker_text, writer_headers, writer_text])\n",
    "    display(Markdown(raw))\n",
    "\n",
    "def see_thinker_tokens():\n",
    "    print(text_prompt_t, end=\"\")\n",
    "    print(tokenizer.decode(tokens_by_worker[0]), end=\"\")\n",
    "    print(text_split_t, end=\"\")\n",
    "    \n",
    "    print(tokenizer.decode(tokens_by_worker[1]), end=\"\")\n",
    "    print(tokenizer.decode([new_tok_t]), end=\"\")\n",
    "\n",
    "def see_writer_tokens():\n",
    "    print(text_prompt_w, end=\"\")\n",
    "    print(tokenizer.decode(tokens_by_worker[1]), end=\"\")\n",
    "    print(text_split_w, end=\"\")\n",
    "    \n",
    "    print(tokenizer.decode(tokens_by_worker[0]), end=\"\")\n",
    "    print(tokenizer.decode([new_tok_w]), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5a34f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefill_cache_block(text: str, blocks, write_to=None):\n",
    "    if write_to is None:\n",
    "        write_to = blocks[-1]\n",
    "    tmp_cm = shared_cache.SharedCacheManager(cache_structure=[blocks], write_to=[write_to])\n",
    "    encoded = tokenizer(text, **tokenizer_kwargs)[\"input_ids\"].to(device)\n",
    "    with torch.inference_mode():\n",
    "        model(**tmp_cm.get_input_kwargs(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "056bf582",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_w, prompt_t, split_w, split_t, cache_w, cache_t, starter_w_for_init_t, starter_t_for_init_w = (\n",
    "    shared_cache.CacheBlock(config=model.config) for _ in range(8)\n",
    ")\n",
    "\n",
    "cm_thinker_only = shared_cache.SharedCacheManager(\n",
    "    cache_structure=[[prompt_t, cache_w, split_t, cache_t]],\n",
    "    write_to=[cache_t],\n",
    ")\n",
    "\n",
    "cm_writer_only = shared_cache.SharedCacheManager(\n",
    "    cache_structure=[[prompt_w, cache_t, split_w, cache_w]],\n",
    "    write_to=[cache_w],\n",
    ")\n",
    "\n",
    "cm_thinking_and_writing = shared_cache.SharedCacheManager(\n",
    "    cache_structure=[\n",
    "        [prompt_w, cache_t, split_w, cache_w],\n",
    "        [prompt_t, cache_w, split_t, cache_t],\n",
    "    ],\n",
    "    write_to=[cache_w, cache_t],\n",
    ")\n",
    "\n",
    "prefill_cache_block(text_prompt_w, [prompt_w])\n",
    "prefill_cache_block(text_prompt_t, [prompt_t])\n",
    "prefill_cache_block(worker_prompts[1], [prompt_w, starter_t_for_init_w])\n",
    "prefill_cache_block(worker_prompts[0], [prompt_t, starter_w_for_init_t])\n",
    "prefill_cache_block(text_split_w, [prompt_w, starter_t_for_init_w, split_w])\n",
    "prefill_cache_block(text_split_t, [prompt_t, starter_w_for_init_t, split_t])\n",
    "\n",
    "prefill_cache_block(worker_prompts[0], [prompt_w, starter_t_for_init_w, split_w, cache_w])\n",
    "prefill_cache_block(worker_prompts[1], [prompt_t, starter_w_for_init_t, split_t, cache_t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "69f5fc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# thinking_and_writing\n",
       "\n",
       "## Thinker mode\n",
       "\n",
       "\n",
       "<think>I am in Thinker mode. My text is not visible to the user. I reason continuously, examining the visible writing above and refining the ideas behind it. I detect errors, test assumptions, and plan improvements. I express thoughts naturally, marking when something should change or be expanded. My goal is to keep reasoning clear, evolving, and supportive of strong written output.\n",
       "\n",
       "Okay, the user wants me to calculate the expression x - x^2 + x^3 for x = 5, 6, 7, and 8, and return all four answers in boxed notation. Let me start by understanding the problem.\n",
       "\n",
       "\n",
       "<control> Have I thought enough to write small next steps? (yes/no): yes\n",
       "\n",
       "Okay, the user wants me to calculate x - x^2 + x^3 for x = 5, 6, 7, and 8, and return all four answers in boxed notation. Let me start by understanding the problem.\n",
       "\n",
       "First, I need to evaluate the expression $ x - x^2 + x^3 $ for each of the given values of $ x $. Let me start with $ x = 5 $.\n",
       "\n",
       "For $ x = 5 $:\n",
       "$$\n",
       "5 - 5^2 + 5^3 = 5 - 25 + 125 = 105\n",
       "$$\n",
       "\n",
       "Next, for $ x = 6 $:\n",
       "$$\n",
       "6 - 6^2 + 6^3 = 6 - 36 + 216 = 186\n",
       "$$\n",
       "\n",
       "For $ x = 7 $:\n",
       "$$\n",
       "7 - 7^2 + 7^3 = 7 - 49 + 343 = 299\n",
       "$$\n",
       "\n",
       "For $ x = 8 $:\n",
       "$$\n",
       "8 - 8^2 + 8^3 = 8 - 64 + 512 = 456\n",
       "$$\n",
       "\n",
       "The results are $ 105 $, $ 186 $, $ 299 $, and $ 456 $ for $ x = 5 $, $ 6 $, $ 7 $, and $ 8 $, respectively. The final answers are:\n",
       "\n",
       "## Writer mode\n",
       "\n",
       "\n",
       "I am in Writer mode. My text is visible to the user. I focus on clear, precise expression and careful word choice. I write only what is well-reasoned and verified in my workspace. I never speculate or improvise. If my thinking shifts or reveals an error, I immediately adjust. My goal is calm, accurate, and readable output.\n",
       "\n",
       "<the writer will continue here> \n",
       "\n",
       "\n",
       "<control> Have I thought enough to continuing writing next step? (yes/no): yes\n",
       "\n",
       "Let's calculate the expression $ x - x^2 + x^3 $ for each value of $ x $:\n",
       "\n",
       "\n",
       "<control> Have I thought enough to continuing writing next step? (yes/no): yes\n",
       "\n",
       "For $ x = 5 $:\n",
       "$$\n",
       "5 - 5^2 + 5^3 = 5 - 25 + 125 = 105\n",
       "$$\n",
       "\n",
       "\n",
       "<control> Have I thought enough to continuing writing next step? (yes/no): yes\n",
       "\n",
       "For $ x = 6 $:\n",
       "$$\n",
       "6 - 6^2 + 6^3 = 6 - 36 + 216 = 186\n",
       "$$\n",
       "For $ x = 7 $:\n",
       "$$\n",
       "7 - 7^2 + 7^3 = 7 - 49 + 343 = 299\n",
       "$$\n",
       "For $ x = 8 $:\n",
       "$$\n",
       "8 - 8^2 + 8^3 = 8 - 64 + 512 = 456\n",
       "$$\n",
       "The results are $ 105 $, $ 186 $, $ 299 $, and $ 456 $ for $ x = 5 $, $ 6 $, $ 7 $, and $ 8 $, respectively. The final answers are: $\\boxed{105}$, $\\boxed{186}$, $\\boxed{299}$, and $\\boxed{456}$. \n",
       "\n",
       "\n",
       "<control> Have I thought enough to continuing writing next step? (yes/no): yes\n",
       "\n",
       "The results are $ 105 $, $ 186 $, $ 299 $, and $"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m next_inputs = {\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: torch.tensor([writer_tokens[-\u001b[32m1\u001b[39m:], thinker_tokens[-\u001b[32m1\u001b[39m:]], device=device)}\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcm_thinking_and_writing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_input_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.logits[..., -\u001b[32m1\u001b[39m, :]\n\u001b[32m     49\u001b[39m     logits[..., forbidden_token_ix] -= \u001b[32m100\u001b[39m \u001b[38;5;66;03m# TODO do better\u001b[39;00m\n\u001b[32m     50\u001b[39m new_tok_w, new_tok_t = logits.argmax(-\u001b[32m1\u001b[39m).tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:850\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m output_hidden_states = (\n\u001b[32m    846\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    847\u001b[39m )\n\u001b[32m    849\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    864\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:576\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    565\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    566\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    573\u001b[39m         position_embeddings,\n\u001b[32m    574\u001b[39m     )\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    588\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:289\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    302\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:222\u001b[39m, in \u001b[36mQwen3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    221\u001b[39m cos, sin = position_embeddings\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m query_states, key_states = \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    225\u001b[39m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[32m    226\u001b[39m     cache_kwargs = {\u001b[33m\"\u001b[39m\u001b[33msin\u001b[39m\u001b[33m\"\u001b[39m: sin, \u001b[33m\"\u001b[39m\u001b[33mcos\u001b[39m\u001b[33m\"\u001b[39m: cos, \u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m: cache_position}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:128\u001b[39m, in \u001b[36mapply_rotary_pos_emb\u001b[39m\u001b[34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[39m\n\u001b[32m    126\u001b[39m sin = sin.unsqueeze(unsqueeze_dim)\n\u001b[32m    127\u001b[39m q_embed = (q * cos) + (rotate_half(q) * sin)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m k_embed = (k * cos) + (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m * sin)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:98\u001b[39m, in \u001b[36mrotate_half\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     94\u001b[39m         down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrotate_half\u001b[39m(x):\n\u001b[32m     99\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Rotates half the hidden dims of the input.\"\"\"\u001b[39;00m\n\u001b[32m    100\u001b[39m     x1 = x[..., : x.shape[-\u001b[32m1\u001b[39m] // \u001b[32m2\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "show_control_in_ouput = True\n",
    "\n",
    "# All of them are already in cache\n",
    "writer_tokens, thinker_tokens = tokenizer(worker_prompts, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "# These one are next tokens\n",
    "writer_tokens.append(tokenizer(\"\\n\\n\")[\"input_ids\"][0])\n",
    "thinker_tokens.append(tokenizer(\"\\n\\n\")[\"input_ids\"][0])\n",
    "tokens_by_worker = [writer_tokens, thinker_tokens]\n",
    "\n",
    "generated_tokens = [0, 0]\n",
    "\n",
    "state = \"thinker_only\"\n",
    "for step in range(1024):\n",
    "    if state == \"thinker_only\":\n",
    "        next_inputs = {\"input_ids\": torch.tensor([thinker_tokens[-1:]], device=device)}\n",
    "        with torch.inference_mode():\n",
    "            logits = model(**cm_thinker_only.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "            logits[..., forbidden_token_ix] -= 100 # TODO do better\n",
    "        new_tok_t = int(logits.argmax(-1))\n",
    "        if new_tok_t == 151668: # </think> TODO do better\n",
    "            state = \"writer_only\"\n",
    "            continue\n",
    "        thinker_tokens.append(new_tok_t)\n",
    "        generated_tokens[1] += 1\n",
    "\n",
    "        if generated_tokens[1] and ends_with_dnl(thinker_tokens):\n",
    "            logger.debug(f'[CONTROL] from thinker')\n",
    "            ci = tokenizer(thinker_control_q, return_tensors=\"pt\").to(device)\n",
    "            with torch.inference_mode():\n",
    "                logits_ctrl = model(**cm_thinker_only.get_input_kwargs(**ci)).logits[..., -1, :]\n",
    "\n",
    "            decision = decision_yes_no(logits_ctrl)\n",
    "\n",
    "            ans = f\" {decision}\\n\\n\"\n",
    "            ai = tokenizer(ans, return_tensors=\"pt\").to(device)\n",
    "            with torch.inference_mode():\n",
    "                model(**cm_thinker_only.get_input_kwargs(**ai))\n",
    "            text_tokens_to_output = thinker_control_q + ans if show_control_in_ouput else \"\\n\\n\"\n",
    "            thinker_tokens.extend(tokenizer.encode(text_tokens_to_output, add_special_tokens=False))\n",
    "\n",
    "            if decision == \"yes\":\n",
    "                state = \"thinking_and_writing\"\n",
    "\n",
    "    elif state == \"thinking_and_writing\":\n",
    "        next_inputs = {\"input_ids\": torch.tensor([writer_tokens[-1:], thinker_tokens[-1:]], device=device)}\n",
    "        with torch.inference_mode():\n",
    "            logits = model(**cm_thinking_and_writing.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "            logits[..., forbidden_token_ix] -= 100 # TODO do better\n",
    "        new_tok_w, new_tok_t = logits.argmax(-1).tolist()\n",
    "        \n",
    "        if new_tok_t == 151668: # </think> TODO do better\n",
    "            state = \"writer_only\"\n",
    "            writer_tokens.append(new_tok_w)\n",
    "            generated_tokens[0] += 1\n",
    "            continue\n",
    "\n",
    "        writer_tokens.append(new_tok_w)\n",
    "        thinker_tokens.append(new_tok_t)\n",
    "        generated_tokens[0] += 1\n",
    "        generated_tokens[1] += 1\n",
    "\n",
    "        if generated_tokens[0] and ends_with_dnl(writer_tokens):\n",
    "            logger.debug(f'[CONTROL] from writer')\n",
    "            control_texts = [writer_control_q, tokenizer.decode([thinker_tokens[-1]])]\n",
    "            ci = tokenizer(control_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "            with torch.inference_mode():\n",
    "                logits_ctrl = model(**cm_thinking_and_writing.get_input_kwargs(**ci)).logits[..., -1, :]\n",
    "\n",
    "            decision = decision_yes_no(logits_ctrl[0])\n",
    "\n",
    "            ans = f\" {decision}\\n\\n\"\n",
    "            ai = tokenizer([ans, tokenizer.decode([thinker_tokens[-1]])], return_tensors=\"pt\", padding=True).to(device)\n",
    "            with torch.inference_mode():\n",
    "                model(**cm_thinking_and_writing.get_input_kwargs(**ai))\n",
    "\n",
    "            text_tokens_to_output = writer_control_q + ans if show_control_in_ouput else \"\\n\\n\"\n",
    "            writer_tokens.extend(tokenizer.encode(text_tokens_to_output, add_special_tokens=False))\n",
    "\n",
    "            if decision == \"no\":\n",
    "                state = \"thinker_only\"\n",
    "    else: # state == \"writer_only\"\n",
    "        next_inputs = {\"input_ids\": torch.tensor([writer_tokens[-1:]], device=device)}\n",
    "        with torch.inference_mode():\n",
    "            logits = model(**cm_writer_only.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "        new_tok_w = int(logits.argmax(-1))\n",
    "        writer_tokens.append(new_tok_w)\n",
    "        generated_tokens[0] += 1\n",
    "\n",
    "    display_tokens(tokens_by_worker, state)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0e9dcec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "You are in Thinker mode.\n",
      "\n",
      "Your task is to reason continue the incomplete answer above in the best way.\n",
      "\n",
      "Reason continuously in small steps. Explore the problem. Test assumptions. Refine understanding. Use the visible text above as context. Check clarity, accuracy, completeness, and safety. Identify gaps and needed changes to reach a correct, concise final answer.\n",
      "\n",
      "Write thoughts freely and naturally. They do not need to be polished. Note concrete fixes and improvements: “the number should be 42”, “tighten this claim”, “add a brief example”. Provide structure and key points for the next writing pass.\n",
      "\n",
      "Do not speak to the user. This space is for analysis, correction, and planning.\n",
      "\n",
      "Sometimes you are asked to continue writing output. Say \"yes\" if you think there is enogh information to write short next step in problem's solution.\n",
      "\n",
      "You are not the one giving the answer. Wait until the answer is given in previous turn. Until then, double-check your previous output.\n",
      "\n",
      "Goal: maintain a continuous, self-correcting flow of reasoning that directs how to finish the answer above correctly and efficiently.\n",
      "\n",
      "Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\boxed{ }.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I am in Writer mode. My text is visible to the user. I focus on clear, precise expression and careful word choice. I write only what is well-reasoned and verified in my workspace. I never speculate or improvise. If my thinking shifts or reveals an error, I immediately adjust. My goal is calm, accurate, and readable output.\n",
      "\n",
      "<the writer will continue here> \n",
      "\n",
      "\n",
      "<control> Have I thought enough to continuing writing next step? (yes/no): yes\n",
      "\n",
      "Let's calculate the expression $ x - x^2 + x^3 $ for each value of $ x $:\n",
      "\n",
      "\n",
      "<control> Have I thought enough to continuing writing next step? (yes/no): yes\n",
      "\n",
      "For $ x = 5 $:\n",
      "$$\n",
      "5 - 5^2 + 5^3 = 5 - 25 + 125 = 105\n",
      "$$\n",
      "\n",
      "\n",
      "<control> Have I thought enough to continuing writing next step? (yes/no): yes\n",
      "\n",
      "For $ x = 6 $:\n",
      "$$\n",
      "6 - 6^2 + 6^3 = 6 - 36 + 216 = 186\n",
      "$$\n",
      "For $ x = 7 $:\n",
      "$$\n",
      "7 - 7^2 + 7^3 = 7 - 49 + 343 = 299\n",
      "$$\n",
      "For $ x = 8 $:\n",
      "$$\n",
      "8 - 8^2 + 8^3 = 8 - 64 + 512 = 456\n",
      "$$\n",
      "The results are $ 105 $, $ 186 $, $ 299 $, and $ 456 $ for $ x = 5 $, $ 6 $, $ 7 $, and $ 8 $, respectively. The final answers are: $\\boxed{105}$, $\\boxed{186}$, $\\boxed{299}$, and $\\boxed{456}$. \n",
      "\n",
      "\n",
      "<control> Have I thought enough to continuing writing next step? (yes/no): yes\n",
      "\n",
      "The results are $ 105 $, $ 186 $, $ 299 $, and $ <the writer will continue here>\n",
      "\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>I am in Thinker mode. My text is not visible to the user. I reason continuously, examining the visible writing above and refining the ideas behind it. I detect errors, test assumptions, and plan improvements. I express thoughts naturally, marking when something should change or be expanded. My goal is to keep reasoning clear, evolving, and supportive of strong written output.\n",
      "\n",
      "Okay, the user wants me to calculate the expression x - x^2 + x^3 for x = 5, 6, 7, and 8, and return all four answers in boxed notation. Let me start by understanding the problem.\n",
      "\n",
      "\n",
      "<control> Have I thought enough to write small next steps? (yes/no): yes\n",
      "\n",
      "Okay, the user wants me to calculate x - x^2 + x^3 for x = 5, 6, 7, and 8, and return all four answers in boxed notation. Let me start by understanding the problem.\n",
      "\n",
      "First, I need to evaluate the expression $ x - x^2 + x^3 $ for each of the given values of $ x $. Let me start with $ x = 5 $.\n",
      "\n",
      "For $ x = 5 $:\n",
      "$$\n",
      "5 - 5^2 + 5^3 = 5 - 25 + 125 = 105\n",
      "$$\n",
      "\n",
      "Next, for $ x = 6 $:\n",
      "$$\n",
      "6 - 6^2 + 6^3 = 6 - 36 + 216 = 186\n",
      "$$\n",
      "\n",
      "For $ x = 7 $:\n",
      "$$\n",
      "7 - 7^2 + 7^3 = 7 - 49 + 343 = 299\n",
      "$$\n",
      "\n",
      "For $ x = 8 $:\n",
      "$$\n",
      "8 - 8^2 + 8^3 = 8 - 64 + 512 = 456\n",
      "$$\n",
      "\n",
      "The results are $ 105 $, $ 186 $, $ 299 $, and $ 456 $ for $ x = 5 $, $ 6 $, $ 7 $, and $ 8 $, respectively. The final answers are::"
     ]
    }
   ],
   "source": [
    "see_thinker_tokens()\n",
    "# see_writer_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a869e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151668]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"</think>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa83e3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1874708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Permanent parallel two-stream generation\n",
    "\n",
    "# next_inputs = tokenizer(worker_prompts, **tokenizer_kwargs).to(device)\n",
    "# tokens_by_worker = tokenizer(worker_prompts, add_special_tokens=False)[\"input_ids\"]\n",
    "# for inference_step in range(1024):       # <-- change max tokens here\n",
    "#     with torch.inference_mode():\n",
    "#         logits = model(**cm_thinking_and_writing.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "#         logits[..., forbidden_token_ix] -= 100\n",
    "#         new_tokens = logits.argmax(-1)   # <-- greedy generation\n",
    "#         next_inputs = dict(input_ids=new_tokens.view(-1, 1))\n",
    "    \n",
    "#     for worker_tokens, new_token in zip(tokens_by_worker, new_tokens.tolist()):\n",
    "#         worker_tokens.append(new_token)\n",
    "#     clear_output(True)\n",
    "#     display(Markdown(\"\".join(tokenizer.decode(seq) for seq in tokens_by_worker[::-1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AsyncReasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
