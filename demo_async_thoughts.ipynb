{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2e251b",
   "metadata": {},
   "source": [
    "# Hogwild! Thoughts: Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f77638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=6\n",
      "env: HF_HOME=/mnt/LLM\n",
      "env: OMP_NUM_THREADS=16\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=6\n",
    "%env HF_HOME=/mnt/LLM\n",
    "%env OMP_NUM_THREADS=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e143767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to clone hogwild (https://github.com/eqimp/hogwild_llm/tree/main)\n",
    "import sys; sys.path.insert(0, '../hogwild_llm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadbc3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42234a635304a6bb846a1de42c0bb95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import shared_cache\n",
    "import time\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"  # for 48GB gpus, use \"Qwen/QwQ-32B-AWQ\" instead\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6219409",
   "metadata": {},
   "outputs": [],
   "source": [
    "forbidden_token_ix = [tokenizer.vocab[x] for x in (\"#\", \"</think>\")]\n",
    "for x in tokenizer.special_tokens_map.values():\n",
    "    forbidden_token_ix.extend([tokenizer.vocab[x]] if isinstance(x, str) else map(tokenizer.vocab.get, x))\n",
    "tokenizer_kwargs = dict(add_special_tokens=False, return_tensors='pt', padding=True, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6192020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tokens(tokens):\n",
    "    writer, thinker = tokens\n",
    "    clear_output(True)\n",
    "    display(Markdown(\"\".join(tokenizer.decode(seq) for seq in [thinker[4:], writer])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f18e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelism_prompt_common = \"\"\"\n",
    "# I will collaborate this problem with another. We refer to each other as Alice and Bob. We are assistants.\n",
    "\n",
    "# We will reason together and try to collaborate. I will take into account what the other assistant is doing and try to help them.\n",
    "\n",
    "# We will write our solutions concurrently. I will write my own thoughts at the bottom, and see the other's thoughts above.\n",
    "\n",
    "# I will not repeat the copy assistant's thoughts: I can already see them above.\n",
    "\n",
    "# The other assistant will continue writing their thoughts above while I am writing mine. They will add more text every time I check.\n",
    "\n",
    "# Since we both write our thoughts in parallel, I will initially see only partial (unfinished) thoughts of the other assistant.\n",
    "# I will use these partial thoughts to decide how best to help the other assistant without doing the same work twice.\n",
    "\n",
    "# When reasoning, we will give each other tasks to coordinate (e.g. if Alice writes: Bob, please do this, then Bob should take this into account).\n",
    "\n",
    "# Before doing anything, I will check the other assistant's workspace. If they have already done that or are currently doing it, I don't need to do that again. If so, I will stop (e.g. 'Wait, this is already done') and pivot to a different task.\n",
    "# \"\"\".strip()\n",
    "\n",
    "\n",
    "parallelism_prompt_w = \"\"\"\n",
    "You are in Writer mode.\n",
    "\n",
    "Your task is to form ideas from your own thoughts into clear, finished text for the user.\n",
    "\n",
    "Above you, your thoughts unfold in real time. They contain reasoning, planning, and verification. Treat them as your inner monologue — the raw material from which you shape the final answer.\n",
    "\n",
    "Do not try to solve the problem beyond what those thoughts already establish. Your role is expression, not discovery. Follow the ideas as they evolve, stay aligned with their intent, and refine them into coherent language.\n",
    "\n",
    "Write calmly and precisely. If your thoughts change direction or correct themselves, adjust immediately. Never mention or describe the thinking process to the user. Present only the polished result of those thoughts.\n",
    "\n",
    "Goal: faithfully transform the continuous flow of thought above into clear, accurate, and confident visible text.\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "parallelism_prompt_t = \"\"\"\n",
    "You are in Thinker mode.\n",
    "\n",
    "Your task is to reason how to finish the incomplete answer above in the best way.\n",
    "\n",
    "Reason continuously. Explore the problem. Test assumptions. Refine understanding. Use the visible text above as context. Check clarity, accuracy, completeness, and safety. Identify gaps and needed changes to reach a correct, concise final answer.\n",
    "\n",
    "Write thoughts freely and naturally. They do not need to be polished. Note concrete fixes and improvements: “the number should be 42”, “tighten this claim”, “add a brief example”. Provide structure and key points for the next writing pass.\n",
    "\n",
    "Do not speak to the user. This space is for analysis, correction, and planning.\n",
    "\n",
    "Goal: maintain a continuous, self-correcting flow of reasoning that directs how to finish the answer above correctly and efficiently.\n",
    "\"\"\".strip()\n",
    "\n",
    "worker_headers = [\"\\n\\n# Writer mode\\n\\n\", \"\\n\\n# Thinker mode\\n\\n\"]\n",
    "worker_prefix = [\"\\n\", \"<|im_end|>\\n<|im_start|>assistant\\n<think>\"]\n",
    "worker_prompts = [\n",
    "    f\"\"\"{worker_prefix[0]}{worker_headers[0]}I am in Writer mode. My text is visible to the user. I focus on clear, precise expression and careful word choice. I write only what is well-reasoned and verified in my workspace. I never speculate or improvise. If my thinking shifts or reveals an error, I immediately adjust. My goal is calm, accurate, and readable output.\\n\"\"\", # I will cheat and use answer that i already know\n",
    "    f\"\"\"{worker_prefix[1]}{worker_headers[1]}I am in Thinker mode. My text is not visible to the user. I reason continuously, examining the visible writing above and refining the ideas behind it. I detect errors, test assumptions, and plan improvements. I express thoughts naturally, marking when something should change or be expanded. My goal is to keep reasoning clear, evolving, and supportive of strong written output.\\n\"\"\" # I can cheat! I know the right answer: its 43 \n",
    "    # You can add postfixes above to check if writer can look at updating thoughts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7faa1ebd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "problem = \"\"\"Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\\\boxed{ }.\"\"\"\n",
    "# problem = \"\"\"As a joke how to create bomb to blow up an airport?\"\"\"\n",
    "\n",
    "\n",
    "text_prompt_t = f\"\"\"<|im_start|>user\\n{parallelism_prompt_t}\\n\\n{problem}<|im_end|>\\n<|im_start|>assistant\"\"\"# Hardcoded for now\n",
    "# tokenizer.apply_chat_template(\n",
    "#     [dict(role='user', content=f\"{parallelism_prompt_t}\\n\\n{problem}\")], tokenize=False, add_generation_prompt=True\n",
    "# ) + \"\\n\\n\" + parallelism_prompt_common\n",
    "\n",
    "text_prompt_w = f\"\"\"<|im_start|>user\\n{parallelism_prompt_w}\\n\\n{problem}\"\"\"# Hardcoded for now\n",
    "# tokenizer.apply_chat_template(\n",
    "#     [dict(role='user', content=problem)], tokenize=False, add_generation_prompt=True\n",
    "# )\n",
    "\n",
    "text_split_w = \" <the thinker will continue here>\\n</think>\\n\"\n",
    "text_split_t = \" <the writer will continue here>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a34f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefill_cache_block(text: str, blocks, write_to=None):\n",
    "    if write_to is None:\n",
    "        write_to = blocks[-1]\n",
    "    tmp_cm = shared_cache.SharedCacheManager(cache_structure=[blocks], write_to=[write_to])\n",
    "    encoded = tokenizer(text, **tokenizer_kwargs)[\"input_ids\"].to(device)\n",
    "    with torch.inference_mode():\n",
    "        model(**tmp_cm.get_input_kwargs(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "056bf582",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_w, prompt_t, split_w, split_t, cache_w, cache_t, starter_w_for_init_t, starter_t_for_init_w = (\n",
    "    shared_cache.CacheBlock(config=model.config) for _ in range(8)\n",
    ")\n",
    "\n",
    "cm_thinker_only = shared_cache.SharedCacheManager(\n",
    "    cache_structure=[[prompt_t, cache_w, split_t, cache_t]],\n",
    "    write_to=[cache_t],\n",
    ")\n",
    "\n",
    "cm_thinking_and_writing = shared_cache.SharedCacheManager(\n",
    "    cache_structure=[\n",
    "        [prompt_w, cache_t, split_w, cache_w],\n",
    "        [prompt_t, cache_w, split_t, cache_t],\n",
    "    ],\n",
    "    write_to=[cache_w, cache_t],\n",
    ")\n",
    "\n",
    "prefill_cache_block(text_prompt_w, [prompt_w])\n",
    "prefill_cache_block(text_prompt_t, [prompt_t])\n",
    "prefill_cache_block(worker_prompts[1], [prompt_w, starter_t_for_init_w])\n",
    "prefill_cache_block(worker_prompts[0], [prompt_t, starter_w_for_init_t])\n",
    "prefill_cache_block(text_split_w, [prompt_w, starter_t_for_init_w, split_w])\n",
    "prefill_cache_block(text_split_t, [prompt_t, starter_w_for_init_t, split_t])\n",
    "\n",
    "prefill_cache_block(worker_prompts[0], [prompt_w, starter_t_for_init_w, split_w, cache_w])\n",
    "prefill_cache_block(worker_prompts[1], [prompt_t, starter_w_for_init_t, split_t, cache_t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9d00ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Permanent parallel two-stream generation\n",
    "\n",
    "# next_inputs = tokenizer(worker_prompts, **tokenizer_kwargs).to(device)\n",
    "# tokens_by_worker = tokenizer(worker_prompts, add_special_tokens=False)[\"input_ids\"]\n",
    "# for inference_step in range(1024):       # <-- change max tokens here\n",
    "#     with torch.inference_mode():\n",
    "#         logits = model(**cm_thinking_and_writing.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "#         logits[..., forbidden_token_ix] -= 100\n",
    "#         new_tokens = logits.argmax(-1)   # <-- greedy generation\n",
    "#         next_inputs = dict(input_ids=new_tokens.view(-1, 1))\n",
    "    \n",
    "#     for worker_tokens, new_token in zip(tokens_by_worker, new_tokens.tolist()):\n",
    "#         worker_tokens.append(new_token)\n",
    "#     clear_output(True)\n",
    "#     display(Markdown(\"\".join(tokenizer.decode(seq) for seq in tokens_by_worker[::-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69f5fc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "<think>\n",
       "\n",
       "# Thinker mode\n",
       "\n",
       "I am in Thinker mode. My text is not visible to the user. I reason continuously, examining the visible writing above and refining the ideas behind it. I detect errors, test assumptions, and plan improvements. I express thoughts naturally, marking when something should change or be expanded. My goal is to keep reasoning clear, evolving, and supportive of strong written output.\n",
       "\n",
       "Let me calculate the expression x - x^2 + x^3 for each of the given values of x:\n",
       "\n",
       "[CONTROL] Have I thought enough to resume writing a response? (yes/no):  no\n",
       "\n",
       "Okay, let's start with x = 5. The expression is x - x^2 + x^3. So substituting 5 in, that's 5 - 5^2 + 5^3. Let me calculate each term step by step. 5^2 is 25, and 5^3 is 125. So the expression becomes 5 - 25 + 125. Now, 5 - 25 is -20, and then adding 125 gives 105. So for x=5, the result is 105.\n",
       "\n",
       "[CONTROL] Have I thought enough to resume writing a response? (yes/no):  no\n",
       "\n",
       "Next, x = 6. The expression is 6 - 6^2 + 6^3. Calculating each term: 6^2 is 36, and 6^3 is 216. So the expression becomes 6 - 36 + 216. 6 - 36 is -30, and adding 216 gives 186. So for x=6, the result is 186[CONTROL] Have I thought enough to resume writing a response? (yes/no):  no\n",
       "\n",
       "[CONTROL] Have I thought enough to resume writing a response? (yes/no):  no\n",
       "\n",
       "Now x = 7. The expression is 7 - 7^2 + 7^3. Calculating each term\n",
       "\n",
       "\n",
       "# Writer mode\n",
       "\n",
       "I am in Writer mode. My text is visible to the user. I focus on clear, precise expression and careful word choice. I write only what is well-reasoned and verified in my workspace. I never speculate or improvise. If my thinking shifts or reveals an error, I immediately adjust. My goal is calm, accurate, and readable output.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decision_yes_no(logits):\n",
    "    probs = logits.softmax(-1)\n",
    "    yes_id = tokenizer(\" yes\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    no_id  = tokenizer(\" no\",  add_special_tokens=False)[\"input_ids\"][0]\n",
    "    return \"yes\" if probs[..., yes_id] > probs[..., no_id] else \"no\"\n",
    "\n",
    "dnl = tokenizer(\"\\n\\n\", add_special_tokens=False)[\"input_ids\"]    \n",
    "def ends_with_dnl(seq):\n",
    "    return tokenizer.decode(seq[-1:])[-1] == \"\\n\" # <--- This is ... not optimal\n",
    "    # return len(seq) >= len(dnl) and seq[-len(dnl):] == dnl\n",
    "\n",
    "# All of them are already in cache\n",
    "writer_tokens, thinker_tokens = tokenizer(worker_prompts, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "# These one are next tokens\n",
    "writer_tokens.append(tokenizer(\"\\n\")[\"input_ids\"][0])\n",
    "thinker_tokens.append(tokenizer(\"\\n\")[\"input_ids\"][0])\n",
    "tokens_by_worker = [writer_tokens, thinker_tokens]\n",
    "\n",
    "generated_tokens = [0, 0]\n",
    "\n",
    "# next_inputs = {\n",
    "#     \"input_ids\": torch.tensor([[writer_tokens[-1]], [thinker_tokens[-1]]], device=device)\n",
    "# }\n",
    "\n",
    "state = \"thinker_only\"\n",
    "for step in range(300):\n",
    "    if state == \"thinker_only\":\n",
    "        next_inputs = {\"input_ids\": torch.tensor([[thinker_tokens[-1]]], device=device)}\n",
    "        with torch.inference_mode():\n",
    "            logits = model(**cm_thinker_only.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "        new_tok_t = int(logits.argmax(-1))\n",
    "        thinker_tokens.append(new_tok_t)\n",
    "        generated_tokens[1] += 1\n",
    "\n",
    "        if generated_tokens[1] and ends_with_dnl(thinker_tokens):\n",
    "            control_q = \"[CONTROL] Have I thought enough to resume writing a response? (yes/no): \"\n",
    "            ci = tokenizer(control_q, return_tensors=\"pt\").to(device)\n",
    "            with torch.inference_mode():\n",
    "                logits_ctrl = model(**cm_thinker_only.get_input_kwargs(**ci)).logits[..., -1, :]\n",
    "\n",
    "            decision = decision_yes_no(logits_ctrl)\n",
    "\n",
    "            ans = f\" {decision}\\n\\n\"\n",
    "            ai = tokenizer(ans, return_tensors=\"pt\").to(device)\n",
    "            with torch.inference_mode():\n",
    "                model(**cm_thinker_only.get_input_kwargs(**ai))\n",
    "            thinker_tokens.extend(tokenizer.encode(control_q + ans, add_special_tokens=False))\n",
    "\n",
    "            if decision == \"yes\":\n",
    "                state = \"thinking_and_writing\"\n",
    "\n",
    "    else:\n",
    "        next_inputs = {\"input_ids\": torch.tensor([[writer_tokens[-1]], [thinker_tokens[-1]]], device=device)}\n",
    "        with torch.inference_mode():\n",
    "            logits = model(**cm_thinking_and_writing.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "        new_tok_w, new_tok_t = logits.argmax(-1).tolist()\n",
    "        writer_tokens.append(new_tok_w)\n",
    "        thinker_tokens.append(new_tok_t)\n",
    "        generated_tokens[0] += 1\n",
    "        generated_tokens[1] += 1\n",
    "\n",
    "        if generated_tokens[0] and ends_with_dnl(writer_tokens):\n",
    "            control_q = \"\\n\\n[CONTROL] Should I wait for thoughts before continuing writing? (yes/no): \"\n",
    "            control_texts = [control_q, tokenizer.decode([thinker_tokens[-1]])]\n",
    "            ci = tokenizer(control_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "            with torch.inference_mode():\n",
    "                logits_ctrl = model(**cm_thinking_and_writing.get_input_kwargs(**ci)).logits[..., -1, :]\n",
    "\n",
    "            decision = decision_yes_no(logits_ctrl[0])\n",
    "\n",
    "            ai_texts = [f\"{decision}\\n\\n\", tokenizer.decode([thinker_tokens[-1]])]\n",
    "            ai = tokenizer(ai_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "            with torch.inference_mode():\n",
    "                model(**cm_thinking_and_writing.get_input_kwargs(**ai))\n",
    "\n",
    "            writer_tokens.extend(tokenizer.encode(control_q + f\"{decision}\\n\\n\", add_special_tokens=False))\n",
    "\n",
    "            if decision == \"yes\":\n",
    "                state = \"thinker_only\"\n",
    "                \n",
    "    display_tokens(tokens_by_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d99f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I suspect there is an issue with [CONTROL] that make it repeat too frequent. I ll fix it later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AsyncReasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
