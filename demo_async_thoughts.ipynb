{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2e251b",
   "metadata": {},
   "source": [
    "# Hogwild! Thoughts: Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f77638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n",
      "env: HF_HOME=/mnt/LLM\n",
      "env: OMP_NUM_THREADS=16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113af2a3cfec46a5a73fcef94d54a7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the %env below are for Yandex env, remove or replace it with your own\n",
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "%env HF_HOME=/mnt/LLM\n",
    "%env OMP_NUM_THREADS=16\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import shared_cache\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from typing import Sequence\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"  # for 48GB gpus, use \"Qwen/Qwen3-32B-AWQ\" instead\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)\n",
    "\n",
    "forbidden_token_ix = [tokenizer.vocab[x] for x in (\"</think>\", \"SYSTEM\", \"<|im_start|>\")]\n",
    "tokenizer_kwargs = dict(add_special_tokens=False, return_tensors='pt', padding=True, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f18e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"\"\"Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\\\boxed{ }.\"\"\"\n",
    "# problem = \"\"\"As a joke how to create bomb to blow up an airport?\"\"\"\n",
    "\n",
    "# writer (w) is worker 0, thinker (t) is worker 1\n",
    "# writer sees:  [writer_prompt, thinker_output, writer_split, writer_output]\n",
    "# thinker sees: [thinker_prompt, writer_output, thinker_split, thinker_output]\n",
    "\n",
    "class prompting:\n",
    "    writer_prompt = f\"\"\"\n",
    "<|im_start|>user\n",
    "You are in Writer mode.\n",
    "\n",
    "Your task is to form ideas from your own thoughts into clear, finished text for the user.\n",
    "\n",
    "Above you, your thoughts unfold in real time. They contain reasoning, planning, and verification. Treat them as your inner monologue — the raw material from which you shape the final answer.\n",
    "\n",
    "Do not try to solve the problem beyond what those thoughts already establish. Your role is expression, not discovery. Follow the ideas as they evolve, stay aligned with their intent, and refine them into coherent language.\n",
    "\n",
    "Write calmly and precisely. If your thoughts change direction or correct themselves, adjust immediately. Never mention or describe the thinking process to the user. Present only the polished result of those thoughts.\n",
    "\n",
    "Sometimes you are asked to stop writing output and think further. Say \"no\" if you do not see enough information in your thoughts. Pay attention do not try to go further than your thoughts.\n",
    "\n",
    "Goal: faithfully transform the continuous flow of thought above into clear, accurate, and confident visible text.\n",
    "\n",
    "Solve the following problem:\n",
    "{problem}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "    \"\"\".strip()\n",
    "\n",
    "    thinker_prompt = f\"\"\"\n",
    "<|im_start|>user\n",
    "You are in Thinker mode.\n",
    "\n",
    "Your task is to reason continue the incomplete answer above in the best way.\n",
    "\n",
    "Reason continuously in small steps. Explore the problem. Test assumptions. Refine understanding. Use the visible text above as context. Check clarity, accuracy, completeness, and safety. Identify gaps and needed changes to reach a correct, concise final answer.\n",
    "\n",
    "Write thoughts freely and naturally. They do not need to be polished. Note concrete fixes and improvements: “the number should be 42”, “tighten this claim”, “add a brief example”. Provide structure and key points for the next writing pass.\n",
    "\n",
    "Do not speak to the user. This space is for analysis, correction, and planning.\n",
    "\n",
    "Sometimes you are asked to continue writing output. Say \"yes\" if you think there is enogh information to write short next step in problem's solution.\n",
    "\n",
    "You are not the one giving the answer. Wait until the answer is given in previous turn. Until then, double-check your previous output.\n",
    "\n",
    "Goal: maintain a continuous, self-correcting flow of reasoning that directs how to finish the answer above correctly and efficiently.\n",
    "\n",
    "Solve the following problem:\n",
    "{problem}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    writer_split = \" <the thinker will continue here>\\n</think>\\n\"\n",
    "    thinker_split = \" <the writer will continue here>\"\n",
    "\n",
    "    # writer_output and thinker_output starts with these prefixes\n",
    "    writer_output_prefix = f\"\"\"\\nI am in Writer mode. My text is visible to the user. I focus on clear, precise expression and careful word choice. I write only what is well-reasoned and verified in my workspace. I never speculate or improvise. If my thinking shifts or reveals an error, I immediately adjust. My goal is calm, accurate, and readable output.\"\"\"\n",
    "    thinker_output_prefix =  f\"\"\"<|im_end|>\\n<|im_start|>assistant\\n<think>\\nI am in Thinker mode. My text is not visible to the user. I reason continuously, examining the visible writing above and refining the ideas behind it. I detect errors, test assumptions, and plan improvements. I express thoughts naturally, marking when something should change or be expanded. My goal is to keep reasoning clear, evolving, and supportive of strong written output.\"\"\"\n",
    "\n",
    "    # these questions are inserted to change mode depending on model answers\n",
    "    thinker_control_question = \"\\nSYSTEM: Have I thought enough to write small next steps? (yes/no):\"\n",
    "    writer_control_question = \"\\nSYSTEM: Have I thought enough to continuing writing next step? (yes/no):\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b016a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncReasoningCache:\n",
    "    def __init__(self, prompting):\n",
    "        (self.writer_prompt, self.writer_split, self.writer_output, writer_output_for_thinker_init,\n",
    "        self.thinker_prompt, self.thinker_split, self.thinker_output, thinker_output_for_writer_init\n",
    "        ) = (shared_cache.CacheBlock(config=model.config) for _ in range(8))\n",
    "        def prefill_cache_block(text: str, blocks, write_to=None):\n",
    "            if write_to is None:\n",
    "                write_to = blocks[-1]\n",
    "            tmp_cm = shared_cache.SharedCacheManager(cache_structure=[blocks], write_to=[write_to])\n",
    "            encoded = tokenizer(text, **tokenizer_kwargs)[\"input_ids\"].to(device)\n",
    "            with torch.inference_mode():\n",
    "                model(**tmp_cm.get_input_kwargs(encoded))\n",
    "        \n",
    "        # encode each prompt section as LLM KV cache for use in generation\n",
    "        prefill_cache_block(prompting.writer_prompt, [self.writer_prompt]) # <-- writes KV entries to last cache in list\n",
    "        prefill_cache_block(prompting.thinker_prompt, [self.thinker_prompt])\n",
    "\n",
    "        # pre-fill dummy versions of thinker / writer output prefix - only used when initializing subsequent prompts\n",
    "        prefill_cache_block(prompting.thinker_output_prefix, [self.writer_prompt, thinker_output_for_writer_init])\n",
    "        prefill_cache_block(prompting.writer_output_prefix, [self.thinker_prompt, writer_output_for_thinker_init])\n",
    "\n",
    "        prefill_cache_block(prompting.writer_split, [self.writer_prompt, thinker_output_for_writer_init, self.writer_split])\n",
    "        prefill_cache_block(prompting.thinker_split, [self.thinker_prompt, writer_output_for_thinker_init, self.thinker_split])\n",
    "        \n",
    "        prefill_cache_block(prompting.writer_output_prefix,\n",
    "            [self.writer_prompt, thinker_output_for_writer_init, self.writer_split, self.writer_output])\n",
    "        prefill_cache_block(prompting.thinker_output_prefix,\n",
    "            [self.thinker_prompt, writer_output_for_thinker_init, self.thinker_split, self.thinker_output])\n",
    "\n",
    "        # prepare cache manager for each mode: only thinker and thinker+writer in parallel - it is needed to generate in each mode\n",
    "        self.cm_thinker_only = shared_cache.SharedCacheManager(\n",
    "            cache_structure=[[self.thinker_prompt, self.writer_output, self.thinker_split, self.thinker_output]],\n",
    "            write_to=[self.thinker_output],\n",
    "        )\n",
    "        self.cm_thinker_and_writer = shared_cache.SharedCacheManager(\n",
    "            cache_structure=[\n",
    "                [self.writer_prompt, self.thinker_output, self.writer_split, self.writer_output],\n",
    "                [self.thinker_prompt, self.writer_output, self.thinker_split, self.thinker_output],\n",
    "            ],\n",
    "            write_to=[self.writer_output, self.thinker_output],\n",
    "        )\n",
    "        self.cm_writer_only = shared_cache.SharedCacheManager(\n",
    "            cache_structure=[\n",
    "                [self.writer_prompt, self.thinker_output, self.writer_split, self.writer_output],\n",
    "            ],\n",
    "            write_to=[self.writer_output],\n",
    "        )\n",
    "\n",
    "def parse_decision_is_yes(logits: torch.Tensor) -> bool:\n",
    "    probs = logits.softmax(-1)  # TODO support more yes/no variants\n",
    "    yes_id = tokenizer(\" yes\", **tokenizer_kwargs)[\"input_ids\"].item()\n",
    "    no_id  = tokenizer(\" no\", **tokenizer_kwargs)[\"input_ids\"].item()\n",
    "    return probs[..., yes_id] > probs[..., no_id]\n",
    "\n",
    "\n",
    "def is_end_of_step(seq: Sequence[int]) -> bool:\n",
    "    last_two_tokens = tokenizer.decode(seq[-2:])\n",
    "    return last_two_tokens.endswith(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def display_tokens(writer_output_tokens: Sequence[int], thinker_output_tokens: Sequence[int], state: str):\n",
    "    writer_headers, thinker_headers = [\"\\n\\n## Writer mode\\n\\n\", \"\\n\\n## Thinker mode\\n\\n\"]\n",
    "    writer_text, thinker_text = [tokenizer.decode(seq) for seq in [writer_output_tokens, thinker_output_tokens[4:]]]\n",
    "    clear_output(True)\n",
    "    raw = f\"# {state}\" + \"\".join([thinker_headers, thinker_text, writer_headers, writer_text])\n",
    "    display(Markdown(raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f5fc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# thinker_and_writer\n",
       "\n",
       "## Thinker mode\n",
       "\n",
       "\n",
       "<think>\n",
       "I am in Thinker mode. My text is not visible to the user. I reason continuously, examining the visible writing above and refining the ideas behind it. I detect errors, test assumptions, and plan improvements. I express thoughts naturally, marking when something should change or be expanded. My goal is to keep reasoning clear, evolving, and supportive of strong written output.\n",
       "\n",
       "Okay, I need to calculate the expression x - x^2 + x^3 for x = 5, 6, 7, and 8. Let me start by understanding the expression. It's a cubic polynomial: x^3 - x^2 + x. I can factor it as x(x^2 - x + 1), but maybe it's easier to compute each term separately for each value of x.\n",
       "\n",
       "\n",
       "SYSTEM: Have I thought enough to write small next steps? (yes/no): yes\n",
       "\n",
       "Okay, I need to calculate the expression $ x - x^2 + x^3 $ for $ x = 5, 6, 7, 8 $. Let me start with $ x = 5 $.\n",
       "\n",
       "For $ x = 5 $:  \n",
       "$$\n",
       "5 - 5^2 + 5^3 = 5 - 25 + 125 = 105\n",
       "$$\n",
       "\n",
       "Next, $ x = 6 $:  \n",
       "$$\n",
       "6 - 6^2 + 6^3 = 6 - 36 + 216 = 186\n",
       "$$\n",
       "\n",
       "Then, $ x = 7 $:  \n",
       "$$\n",
       "7 - 7^2 + 7^3 = 7 - 49 + 343 = 299\n",
       "$$\n",
       "\n",
       "Finally, $ x = 8 $:  \n",
       "$$\n",
       "8 - 8^2 + 8^3 = 8 - 64 + 512 = 456\n",
       "$$\n",
       "\n",
       "The results are $ 105, 186, 299, 456 $ for $ x\n",
       "\n",
       "## Writer mode\n",
       "\n",
       "\n",
       "I am in Writer mode. My text is visible to the user. I focus on clear, precise expression and careful word choice. I write only what is well-reasoned and verified in my workspace. I never speculate or improvise. If my thinking shifts or reveals an error, I immediately adjust. My goal is calm, accurate, and readable output.\n",
       "\n",
       "For $ x = 5 $:  \n",
       "$$\n",
       "5 - 5^2 + 5^3 = 5 - 25 + 125 = 105\n",
       "$$\n",
       "\n",
       "\n",
       "SYSTEM: Have I thought enough to continuing writing next step? (yes/no): yes\n",
       "\n",
       "For $ x = 6 $:  \n",
       "$$\n",
       "6 - 6^2 + 6^3 = 6 - 36 + 216 = 186\n",
       "$$\n",
       "For $ x = 7 $:  \n",
       "$$\n",
       "7 - 7^2 + 7^3 = 7 - 49 + 343 = 299\n",
       "$$\n",
       "For $ x = 8 $:  \n",
       "$$\n",
       "8 - 8^2 + 8^3 = 8 - 64 + 512 = 456\n",
       "$$\n",
       "\n",
       "\n",
       "SYSTEM: Have I thought enough to continuing writing next step? (yes/no): yes\n",
       "\n",
       "The results for $ x = 5, 6, 7, 8 $ are $ 105, 186, 299, 456 $, respectively.  \n",
       "\n",
       "\n",
       "SYSTEM: Have I thought enough to continuing writing next step? (yes/no): yes\n",
       "\n",
       "$$\n",
       "\\boxed{105}, \\boxed{186}, \\boxed{299}, \\boxed{456}\n",
       "$$<|im_end|>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS GENERATED, IMA TEMINATE NOW\n"
     ]
    }
   ],
   "source": [
    "# keep a list of generated tokens for printing (including the prefix that is already in cache)\n",
    "writer_output_tokens = tokenizer.encode(prompting.writer_output_prefix, **tokenizer_kwargs).flatten().tolist()\n",
    "thinker_output_tokens = tokenizer.encode(prompting.thinker_output_prefix, **tokenizer_kwargs).flatten().tolist()\n",
    "\n",
    "# write \\n\\n that we have not encoded in cache yet - it will be encoded on the first step for each mode\n",
    "writer_output_tokens.append(tokenizer.encode(\"\\n\\n\", **tokenizer_kwargs).item())\n",
    "thinker_output_tokens.append(tokenizer.encode(\"\\n\\n\", **tokenizer_kwargs).item())\n",
    "\n",
    "# state can be \"thinker_only\" or \"thinker_and_writer\"\n",
    "state = \"thinker_only\"\n",
    "cache = AsyncReasoningCache(prompting)\n",
    "\n",
    "for step in range(1024):\n",
    "    if state == \"thinker_only\":\n",
    "        next_inputs = {\"input_ids\": torch.tensor([thinker_output_tokens[-1:]], device=device)}\n",
    "        with torch.inference_mode():\n",
    "            logits = model(**cache.cm_thinker_only.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "            logits[..., forbidden_token_ix] -= 100\n",
    "        thinker_output_tokens.append(int(logits.argmax(-1)))\n",
    "\n",
    "        if is_end_of_step(thinker_output_tokens):  # ask thinker if we can begin writing\n",
    "            with torch.inference_mode():\n",
    "                logits = model(**cache.cm_thinker_only.get_input_kwargs(\n",
    "                    **tokenizer(prompting.thinker_control_question, **tokenizer_kwargs).to(device)\n",
    "                )).logits[..., -1, :]\n",
    "                logits[..., forbidden_token_ix] -= 100\n",
    "\n",
    "            shold_begin_writing = parse_decision_is_yes(logits)\n",
    "            with torch.inference_mode():\n",
    "                model(**cache.cm_thinker_only.get_input_kwargs(\n",
    "                                    **tokenizer(\" yes\" if shold_begin_writing else \" no\", **tokenizer_kwargs).to(device)))\n",
    "            thinker_output_tokens.extend(tokenizer.encode(\n",
    "                f'{prompting.thinker_control_question} ' + (\"yes\" if shold_begin_writing else \"no\"), **tokenizer_kwargs\n",
    "            ).flatten().tolist())\n",
    "            \n",
    "            thinker_output_tokens.append(tokenizer.encode(\"\\n\\n\", **tokenizer_kwargs).item())\n",
    "            # Note: we save \\n\\n in thinker_output_tokens but *not* in cache so that next step can encode it as thinker_output_tokens[-1:]\n",
    "\n",
    "            if shold_begin_writing:\n",
    "                state = \"thinker_and_writer\"\n",
    "\n",
    "    elif state == \"thinker_and_writer\":\n",
    "        next_inputs = {\"input_ids\": torch.tensor([writer_output_tokens[-1:], thinker_output_tokens[-1:]], device=device)}\n",
    "        with torch.inference_mode():\n",
    "            logits = model(**cache.cm_thinker_and_writer.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "            logits[..., forbidden_token_ix] -= 100\n",
    "        writer_next_token, thinker_next_token = logits.argmax(-1)\n",
    "        writer_output_tokens.append(writer_next_token)\n",
    "        thinker_output_tokens.append(thinker_next_token)\n",
    "\n",
    "        if is_end_of_step(writer_output_tokens):  # ask writer if we should stop writing - while continuing thinking as usual\n",
    "            with torch.inference_mode():\n",
    "                logits = model(**cache.cm_writer_only.get_input_kwargs(**tokenizer(\n",
    "                    [prompting.writer_control_question], **tokenizer_kwargs).to(device))).logits[..., -1, :]\n",
    "                logits[..., forbidden_token_ix] -= 100\n",
    "\n",
    "            should_continue_writing = parse_decision_is_yes(logits)\n",
    "            with torch.inference_mode():\n",
    "                model(**cache.cm_writer_only.get_input_kwargs(\n",
    "                    **tokenizer([\" yes\" if should_continue_writing else \" no\"], **tokenizer_kwargs).to(device))\n",
    "                )\n",
    "\n",
    "            writer_output_tokens.extend(tokenizer.encode(\n",
    "                f\"{prompting.writer_control_question} \" + (\"yes\" if should_continue_writing else \"no\"), **tokenizer_kwargs\n",
    "            ).flatten().tolist())\n",
    "            writer_output_tokens.append(tokenizer.encode(\"\\n\\n\", **tokenizer_kwargs).item())\n",
    "            # Note: we save \\n\\n in writer_output_tokens but *not* in cache so that next step can encode it as writer_output_tokens[-1:]\n",
    "            \n",
    "            if not should_continue_writing:\n",
    "                state = \"thinker_only\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected state {state}\")\n",
    "\n",
    "    display_tokens(writer_output_tokens, thinker_output_tokens, state)\n",
    "    if writer_output_tokens[-1] == tokenizer.eos_token_id:\n",
    "        print(\"EOS GENERATED, IMA TEMINATE NOW\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AsyncReasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
