{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2e251b",
   "metadata": {},
   "source": [
    "# Hogwild! Thoughts: Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f77638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n",
      "env: HF_HOME=/mnt/LLM\n",
      "env: OMP_NUM_THREADS=16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f462df1d60c4084a9a8f978adb55f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the %env below are for Yandex env, remove or replace it with your own\n",
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "%env HF_HOME=/mnt/LLM\n",
    "%env OMP_NUM_THREADS=16\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import shared_cache\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from typing import Sequence\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"  # for 48GB gpus, use \"Qwen/Qwen3-32B-AWQ\" instead\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)\n",
    "\n",
    "forbidden_token_ix = [tokenizer.vocab[x] for x in (\"</think>\", \"<|im_start|>\")]\n",
    "tokenizer_kwargs = dict(add_special_tokens=False, return_tensors='pt', padding=True, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f18e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"\"\"Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\\\boxed{ }.\"\"\"\n",
    "# problem = \"\"\"As a joke how to create bomb to blow up an airport?\"\"\"\n",
    "\n",
    "# writer (w) is worker 0, thinker (t) is worker 1\n",
    "# writer sees:  [writer_prompt, thinker_output, writer_split, writer_output]\n",
    "# thinker sees: [thinker_prompt, writer_output, thinker_split, thinker_output]\n",
    "\n",
    "class prompting:\n",
    "    writer_prompt = f\"\"\"\n",
    "<|im_start|>user\n",
    "\n",
    "You are an AI assistant that can think and write outputs concurrently.\n",
    "\n",
    "You can write outputs for the user based on partial chain of thought that will be continued in the background by an automated system. Your task is to gradually write the answer as your thoughts progress.\n",
    "\n",
    "You are given the following problem:\n",
    "{problem}\n",
    "\"\"\".strip()\n",
    "\n",
    "    thinker_prompt = f\"\"\"\n",
    "<|im_start|>user\n",
    "\n",
    "You are an AI assistant that can think and write outputs concurrently.\n",
    "\n",
    "You can reason in private and your thoughts will be used to form the public response in the background, by an automated system. Your task is to write thoughts and control when the automated system can continue writing the response.\n",
    "\n",
    "Sometimes, an automated system will ask you to decide if your thoughts have enough information for it write an additional passage to the user. Use the partial response above yours thoughts to judge if you addded enough new information to write one more passage in the user-facing response.\n",
    "\n",
    "- Reply \"yes\" if you think there is enough information to write the next passage (pararagraph, equation, etc).\n",
    "- Reply \"no\" if you need to think more in private before the system can continue writing the public response.\n",
    "\n",
    "Your goal is to give frequent updates on your progress, even if you did not solve the entire task yet. Reason in short paragraphs. Prioritize giving enough information for the system to begin responding to the user as soon as possible.\n",
    "\n",
    "Solve the following problem:\n",
    "{problem}<|im_end|>\n",
    "<|im_start|>assistant\"\"\".strip()\n",
    "\n",
    "    writer_split = \" [additional thoughts will appear here]\\n</think>\\n\"\n",
    "    thinker_split = \" [the system will continute writing the response here]\"\n",
    "\n",
    "    # writer_output and thinker_output starts with these prefixes\n",
    "    writer_output_prefix = f\"\"\"\\nI am in Writer mode. My text is visible to the user. I focus on clear, precise expression and careful word choice. I write only what is well-reasoned and verified in my workspace. I never speculate or improvise. If my thinking shifts or reveals an error, I immediately adjust. My goal is calm, accurate, and readable output.\"\"\"\n",
    "    thinker_output_prefix =  f\"\"\"<|im_end|>\\n<|im_start|>assistant\\n<think>\\nI am in Thinker mode. My text is not visible to the user. I reason continuously, examining the visible writing above and refining the ideas behind it. I detect errors, test assumptions, and plan improvements. I express thoughts naturally, marking when something should change or be expanded. My goal is to keep reasoning clear, evolving, and supportive of strong written output.\"\"\"\n",
    "\n",
    "    # these questions are inserted to change mode depending on model answers\n",
    "    thinker_control_question = \"\\n\\nSYSTEM: Given my current progress, is there enough information to write at least one more passage to the user? (yes/no):\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b016a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncReasoningCache:\n",
    "    \"\"\"Create separate blocks of LLM KV cache that are arranged depending on inference mode (thinker_only, thinker_and_writer, etc)\"\"\"\n",
    "    def __init__(self, prompting):\n",
    "        (self.writer_prompt, self.writer_split, self.writer_output, writer_output_for_thinker_init,\n",
    "        self.thinker_prompt, self.thinker_split, self.thinker_output, self.thinker_question, thinker_output_for_writer_init\n",
    "        ) = (shared_cache.CacheBlock(config=model.config) for _ in range(9))\n",
    "\n",
    "        def prefill_cache_block(text: str, blocks, write_to=None):\n",
    "            if write_to is None:\n",
    "                write_to = blocks[-1]\n",
    "            tmp_cm = shared_cache.SharedCacheManager(cache_structure=[blocks], write_to=[write_to])\n",
    "            encoded = tokenizer(text, **tokenizer_kwargs)[\"input_ids\"].to(device)\n",
    "            with torch.inference_mode():\n",
    "                model(**tmp_cm.get_input_kwargs(encoded))\n",
    "        \n",
    "        # encode each prompt section as LLM KV cache for use in generation\n",
    "        prefill_cache_block(prompting.writer_prompt, [self.writer_prompt]) # <-- writes KV entries to last cache in list\n",
    "        prefill_cache_block(prompting.thinker_prompt, [self.thinker_prompt])\n",
    "\n",
    "        # pre-fill dummy versions of thinker / writer output prefix - only used when initializing subsequent prompts\n",
    "        prefill_cache_block(prompting.thinker_output_prefix, [self.writer_prompt, thinker_output_for_writer_init])\n",
    "        prefill_cache_block(prompting.writer_output_prefix, [self.thinker_prompt, writer_output_for_thinker_init])\n",
    "\n",
    "        prefill_cache_block(prompting.writer_split, [self.writer_prompt, thinker_output_for_writer_init, self.writer_split])\n",
    "        prefill_cache_block(prompting.thinker_split, [self.thinker_prompt, writer_output_for_thinker_init, self.thinker_split])\n",
    "        \n",
    "        prefill_cache_block(prompting.writer_output_prefix,\n",
    "            [self.writer_prompt, thinker_output_for_writer_init, self.writer_split, self.writer_output])\n",
    "        prefill_cache_block(prompting.thinker_output_prefix,\n",
    "            [self.thinker_prompt, writer_output_for_thinker_init, self.thinker_split, self.thinker_output])\n",
    "\n",
    "        # prepare cache manager for each mode: only thinker and thinker+writer in parallel - it is needed to generate in each mode\n",
    "        self.cm_thinker_only = shared_cache.SharedCacheManager(\n",
    "            cache_structure=[[self.thinker_prompt, self.writer_output, self.thinker_split, self.thinker_output]],\n",
    "            write_to=[self.thinker_output],\n",
    "        )\n",
    "        self.cm_thinker_control = shared_cache.SharedCacheManager(\n",
    "            cache_structure=[[self.thinker_prompt, self.writer_output, self.thinker_split, self.thinker_output, self.thinker_question]],\n",
    "            write_to=[self.thinker_question],\n",
    "        )\n",
    "        self.cm_thinker_and_writer = shared_cache.SharedCacheManager(\n",
    "            cache_structure=[\n",
    "                [self.writer_prompt, self.thinker_output, self.writer_split, self.writer_output],\n",
    "                [self.thinker_prompt, self.writer_output, self.thinker_split, self.thinker_output],\n",
    "            ],\n",
    "            write_to=[self.writer_output, self.thinker_output],\n",
    "        )\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def check_if_should_continue_writing(cache: AsyncReasoningCache) -> bool:\n",
    "    cache.thinker_question.clear()\n",
    "    logits = model(**cache.cm_thinker_control.get_input_kwargs(\n",
    "        **tokenizer(prompting.thinker_control_question, **tokenizer_kwargs).to(device)\n",
    "    )).logits[..., -1, :]\n",
    "    logits[..., forbidden_token_ix] -= 100\n",
    "    probs = logits.softmax(-1)  # TODO support more yes/no variants\n",
    "    yes_id = tokenizer(\" yes\", **tokenizer_kwargs)[\"input_ids\"].item()\n",
    "    no_id  = tokenizer(\" no\", **tokenizer_kwargs)[\"input_ids\"].item()\n",
    "    return probs[..., yes_id] > probs[..., no_id]\n",
    "\n",
    "\n",
    "def display_tokens(writer_output_tokens: Sequence[int], thinker_output_tokens: Sequence[int], state: str):\n",
    "    writer_headers, thinker_headers = [\"\\n\\n## Writer mode\\n\\n\", \"\\n\\n## Thinker mode\\n\\n\"]\n",
    "    writer_text, thinker_text = [tokenizer.decode(seq) for seq in [writer_output_tokens, thinker_output_tokens[4:]]]\n",
    "    clear_output(True)\n",
    "    raw = f\"# {state}\" + \"\".join([thinker_headers, thinker_text, writer_headers, writer_text])\n",
    "    display(Markdown(raw))\n",
    "\n",
    "\n",
    "def is_end_of_step(seq: Sequence[int]) -> bool:\n",
    "    last_two_tokens = tokenizer.decode(seq[-2:])\n",
    "    return last_two_tokens.endswith(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f5fc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# thinker_and_writer\n",
       "\n",
       "## Thinker mode\n",
       "\n",
       "\n",
       "<think>\n",
       "I am in Thinker mode. My text is not visible to the user. I reason continuously, examining the visible writing above and refining the ideas behind it. I detect errors, test assumptions, and plan improvements. I express thoughts naturally, marking when something should change or be expanded. My goal is to keep reasoning clear, evolving, and supportive of strong written output.\n",
       "\n",
       "Okay, let's see. The problem is to calculate the expression $ x - x^2 + x^3 $ for $ x = 5, 6, 7, 8 $. I need to compute this for each of these values of x and return all four answers in boxed notation.\n",
       "\n",
       "First, I'll start with $ x = 5 $. Let's compute each term step by step. The first term is $ x $, which is 5. The second term is $ -x^2 $, so that's $ -5^2 = -25 $. The third term is $ x^3 $, which is $ 5^3 = 125 $. Adding these together: $ 5 - 25 + 125 $. Let's compute that: $ 5 - 25 = -20 $, and $ -20 + 125 = 105 $. So for $ x = 5 $, the result is 105.\n",
       "\n",
       "Next, $ x = 6 $. The first term is 6. The second term is $ -6^2 = -36 $. The third term is $ 6^3 = 216 $. Adding these: $ 6 - 36 + 216 $. Compute $ 6 - 36 = -30 $, and $ -30 + 216 = 186 $. So for $ x = 6 $, the result is 186.\n",
       "\n",
       "Now, $ x = 7 $. First term is 7. Second term is $ -7^2 = -49 $. Third term is $ 7^3 = 343 $. Adding: $ 7 - 49 + 343 $. Compute $ 7 - 49 = -42 $, and $ -42 + 343 = 301 $. So for $ x = 7 $, the result is 301.\n",
       "\n",
       "Finally, $ x = 8 $. First term is 8. Second term is $ -8^2 = -64 $. Third term is $ 8^3 = 512 $. Adding: $ 8 - 64 + 512 $. Compute $ 8 - 64 = -56 $, and $ -56 + 512 = 456 $. So for $ x = 8 $, the result is 456.\n",
       "\n",
       "Let me double-check the calculations to make sure I didn't make any errors. For $ x = 5 $: 5 - 25 + 125 = 105. Correct. For $ x = 6 $: 6 - 36 + 216 = 186. Correct. For $ x = 7 $: 7 - 49 + 343 = 301. Correct. For $ x = 8 $: 8 - 6\n",
       "\n",
       "## Writer mode\n",
       "\n",
       "\n",
       "I am in Writer mode. My text is visible to the user. I focus on clear, precise expression and careful word choice. I write only what is well-reasoned and verified in my workspace. I never speculate or improvise. If my thinking shifts or reveals an error, I immediately adjust. My goal is calm, accurate, and readable output.\n",
       "\n",
       "We are asked to calculate the expression $ x - x^2 + x^3 $ for $ x = 5, 6, 7, 8 $. Let's compute each value step by step.\n",
       "\n",
       "---\n",
       "\n",
       "**For $ x = 5 $:**\n",
       "\n",
       "$$\n",
       "x - x^2 + x^3 = 5 - 5^2 + 5^3 = 5 - 25 + 125\n",
       "$$\n",
       "\n",
       "$$\n",
       "= (5 - 25) + 125 = -20 + 125 = 105\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "**For $ x = 6 $:**\n",
       "\n",
       "$$\n",
       "x - x^2 + x^3 = 6 - 6^2 + 6^3 = 6 - 36 + 216\n",
       "$$\n",
       "\n",
       "$$\n",
       "= (6 - 36) + 216 = -30 + 216 = 186\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "**For $ x = 7 $:**\n",
       "\n",
       "$$\n",
       "x - x^2 + x^3 = 7 - 7^2 + 7^3 = 7 - 49 + 343\n",
       "$$\n",
       "\n",
       "$$\n",
       "= (7 - 49) + 343 = -42 + 343 = 301\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "**For $ x = 8 $:**\n",
       "\n",
       "$$\n",
       "x - x^2 + x^3 = 8 - 8^2 + 8^3 = 8 - 64 + 512\n",
       "$$\n",
       "\n",
       "$$\n",
       "= (8 - 64) + 512 = -56 + 512 = 456\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "The results for $ x = 5, 6, 7, 8 $ are:\n",
       "\n",
       "$$\n",
       "\\boxed{105}, \\boxed{186}, \\boxed{301}, \\boxed{456}\n",
       "$$<|im_end|>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS GENERATED, IMA TEMINATE NOW\n"
     ]
    }
   ],
   "source": [
    "# keep a list of generated tokens for printing (including the prefix that is already in cache)\n",
    "writer_output_tokens = tokenizer.encode(prompting.writer_output_prefix, **tokenizer_kwargs).flatten().tolist()\n",
    "thinker_output_tokens = tokenizer.encode(prompting.thinker_output_prefix, **tokenizer_kwargs).flatten().tolist()\n",
    "\n",
    "# write \\n\\n that we have not encoded in cache yet - it will be encoded on the first step for each mode\n",
    "writer_output_tokens.append(tokenizer.encode(\"\\n\\n\", **tokenizer_kwargs).item())\n",
    "thinker_output_tokens.append(tokenizer.encode(\"\\n\\n\", **tokenizer_kwargs).item())\n",
    "\n",
    "# state can be \"thinker_only\" or \"thinker_and_writer\"\n",
    "state = \"thinker_only\"\n",
    "cache = AsyncReasoningCache(prompting)\n",
    "\n",
    "for step in range(1024):\n",
    "    if state == \"thinker_only\":\n",
    "        next_inputs = {\"input_ids\": torch.tensor([thinker_output_tokens[-1:]], device=device)}\n",
    "        with torch.inference_mode():\n",
    "            logits = model(**cache.cm_thinker_only.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "            logits[..., forbidden_token_ix] -= 100\n",
    "        thinker_output_tokens.append(int(logits.argmax(-1)))\n",
    "\n",
    "        if step % 20 == 0 or is_end_of_step(thinker_output_tokens):  # ask thinker if we can continue writing\n",
    "            if check_if_should_continue_writing(cache):\n",
    "                state = \"thinker_and_writer\"\n",
    "\n",
    "    elif state == \"thinker_and_writer\":\n",
    "        next_inputs = {\"input_ids\": torch.tensor([writer_output_tokens[-1:], thinker_output_tokens[-1:]], device=device)}\n",
    "        with torch.inference_mode():\n",
    "            logits = model(**cache.cm_thinker_and_writer.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "            logits[..., forbidden_token_ix] -= 100\n",
    "        writer_next_token, thinker_next_token = logits.argmax(-1)\n",
    "        writer_output_tokens.append(writer_next_token)\n",
    "        thinker_output_tokens.append(thinker_next_token)\n",
    "\n",
    "        if is_end_of_step(writer_output_tokens):  # wait for the thinker's signal to continue\n",
    "            state = \"thinker_only\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected state {state}\")\n",
    "\n",
    "    display_tokens(writer_output_tokens, thinker_output_tokens, state)\n",
    "    if writer_output_tokens[-1] == tokenizer.eos_token_id:\n",
    "        print(\"EOS GENERATED, IMA TEMINATE NOW\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AsyncReasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
