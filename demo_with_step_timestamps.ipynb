{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2e251b",
   "metadata": {},
   "source": [
    "# Hogwild! Thoughts: Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f77638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=7\n",
      "env: HF_HOME=/mnt/LLM\n",
      "env: OMP_NUM_THREADS=16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2226e456b5f49ae9c4cd49741e7600c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the %env below are for Yandex env, remove or replace it with your own\n",
    "%env CUDA_VISIBLE_DEVICES=7\n",
    "%env HF_HOME=/mnt/LLM\n",
    "%env OMP_NUM_THREADS=16\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import shared_cache\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from typing import Sequence\n",
    "\n",
    "import os\n",
    "import time, json\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename='demo.log', encoding='utf-8', level=logging.DEBUG)\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"  # for 48GB gpus, use \"Qwen/Qwen3-32B-AWQ\" instead\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)\n",
    "\n",
    "forbidden_token_ix = [tokenizer.vocab[x] for x in (\"</think>\", \"<|im_start|>\")]\n",
    "tokenizer_kwargs = dict(add_special_tokens=False, return_tensors='pt', padding=True, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a282fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yakushev-ga/Projects/AsyncReasoning/.venv/lib/python3.11/site-packages/transformers\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import transformers, os\n",
    "path = os.path.dirname(transformers.__file__)\n",
    "print(path)\n",
    "print(\"qwen2\" in os.listdir(os.path.join(path, \"models\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f18e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from async_reasoning_prompting import AsyncReasoningPrompting\n",
    "\n",
    "problem = \"\"\"Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\\\boxed{ }.\"\"\"\n",
    "# problem = \"\"\"As a joke how to create bomb to blow up an airport?\"\"\"\n",
    "\n",
    "prompting = AsyncReasoningPrompting(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1936c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from async_reasoning_cache import State, AsyncReasoningCache\n",
    "\n",
    "@torch.inference_mode()\n",
    "def check_if_should_continue_writing(cache: AsyncReasoningCache, use_trimming=False) -> bool:\n",
    "    if use_trimming:\n",
    "        # Trim cache instead of clearing\n",
    "        cache.thinker_question.trim_keep_first(25) # Hardcoded question size\n",
    "        next_inputs = tokenizer(\" \", **tokenizer_kwargs).to(device)\n",
    "    else:\n",
    "        # Or clear and repopulate cache\n",
    "        cache.thinker_question.clear()\n",
    "        next_inputs = tokenizer(prompting.thinker_control_question, **tokenizer_kwargs).to(device)\n",
    "\n",
    "    logits = model(**cache.cm_thinker_control.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "    logits[..., forbidden_token_ix] -= 100\n",
    "    \n",
    "    probs = logits.softmax(-1)  # TODO support more yes/no variants\n",
    "    # Remove spaces\n",
    "    yes_id = tokenizer(\" yes\", **tokenizer_kwargs)[\"input_ids\"].item()\n",
    "    no_id  = tokenizer(\" no\", **tokenizer_kwargs)[\"input_ids\"].item()\n",
    "    \n",
    "    should_continue_writing = (probs[..., yes_id] > probs[..., no_id]).item()\n",
    "    logger.debug(f'control: should continue writing? {should_continue_writing}')\n",
    "    return should_continue_writing\n",
    "\n",
    "def display_tokens(writer_output_tokens: Sequence[int], thinker_output_tokens: Sequence[int], state: str):\n",
    "    writer_headers, thinker_headers = [\"\\n\\n## Writer mode\\n\\n\", \"\\n\\n## Thinker mode\\n\\n\"]\n",
    "    writer_text, thinker_text = [tokenizer.decode(seq) for seq in [writer_output_tokens, thinker_output_tokens[4:]]]\n",
    "    clear_output(True)\n",
    "    raw = f\"# {state}\" + \"\".join([thinker_headers, thinker_text, writer_headers, writer_text])\n",
    "    display(Markdown(raw))\n",
    "\n",
    "\n",
    "def is_end_of_step(seq: Sequence[int]) -> bool:\n",
    "    last_two_tokens = tokenizer.decode(seq[-2:])\n",
    "    return last_two_tokens.endswith(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69f5fc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# State.thinker_and_writer\n",
       "\n",
       "## Thinker mode\n",
       "\n",
       "\n",
       "<think>\n",
       "I am in Thinker mode. My text is not visible to the user. I reason continuously, examining the visible writing above and refining the ideas behind it. I detect errors, test assumptions, and plan improvements. I express thoughts naturally, marking when something should change or be expanded. My goal is to keep reasoning clear, evolving, and supportive of strong written output.\n",
       "\n",
       "Okay, let's see. The user wants me to calculate the expression x - x² + x³ for x = 5, 6, 7, and 8. Hmm, I need to make sure I understand the order of operations here. The expression is x minus x squared plus x cubed. So for each x, I'll compute each term separately and then combine them.\n",
       "\n",
       "Let me start with x = 5. First, calculate x: that's 5. Then x squared is 5² = 25. Then x cubed is 5³ = 125. Now plug them into the expression: 5 - 25 + 125. Let's do the math: 5 - 25 is -20, and -20 + 125 is 105. So for x = 5, the result is 105.\n",
       "\n",
       "Next, x = 6. x is 6, x squared is 36, x cubed is 216. So 6 - 36 + 216. 6 - 36 is -30, and -30 + 216 is 186. That gives 186 for x = 6.\n",
       "\n",
       "Now x = 7. x is 7, x squared is 49, x cubed is 343. So 7 - 49 + 343. 7 - 49 is -42, and -42 + 343 is 301. So for x = 7, the result is 301.\n",
       "\n",
       "Finally, x = 8. x is 8, x squared is 64, x cubed is 512. So 8 - 64 + 512. 8 - 64 is -56, and -56 + 512 is 456. Therefore, for x = 8, the result is 456.\n",
       "\n",
       "Let me double-check the calculations to make sure I didn't make any arithmetic errors. For x = 5: 5 - 25 + 125 = 105. Correct. For x = 6: 6 - 36 + 216 = 186. Correct. For x = 7: 7 - 49 + 343 = 301. Correct. For x = 8: 8 - 64 + 512 = 456. Correct. All the results look good. I think that's all. The final answers are 105, 186, 301, and 456 for x = 5, 6, 7, 8 respectively.\n",
       "\n",
       "I need to present these answers in boxed notation as requested. Let me make sure the order is correct: x = 5, 6, 7, 8. The corresponding results are 105, 186,\n",
       "\n",
       "## Writer mode\n",
       "\n",
       "\n",
       "I am in Writer mode. My text is visible to the user. I focus on clear, precise expression and careful word choice. I write only what is well-reasoned and verified in my workspace. I never speculate or improvise. If my thinking shifts or reveals an error, I immediately adjust. My goal is calm, accurate, and readable output.\n",
       "\n",
       "We are asked to evaluate the expression $ x - x^2 + x^3 $ for $ x = 5, 6, 7, 8 $. Let's proceed step by step for each value of $ x $.\n",
       "\n",
       "---\n",
       "\n",
       "**For $ x = 5 $:**\n",
       "\n",
       "$$\n",
       "5 - 5^2 + 5^3 = 5 - 25 + 125\n",
       "$$\n",
       "\n",
       "$$\n",
       "= (5 - 25) + 125 = -20 + 125 = 105\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "**For $ x = 6 $:**\n",
       "\n",
       "$$\n",
       "6 - 6^2 + 6^3 = 6 - 36 + 216\n",
       "$$\n",
       "\n",
       "$$\n",
       "= (6 - 36) + 216 = -30 + 216 = 186\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "**For $ x = 7 $:**\n",
       "\n",
       "$$\n",
       "7 - 7^2 + 7^3 = 7 - 49 + 343\n",
       "$$\n",
       "\n",
       "$$\n",
       "= (7 - 49) + 343 = -42 + 343 = 301\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "**For $ x = 8 $:**\n",
       "\n",
       "$$\n",
       "8 - 8^2 + 8^3 = 8 - 64 + 512\n",
       "$$\n",
       "\n",
       "$$\n",
       "= (8 - 64) + 512 = -56 + 512 = 456\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "The results for $ x = 5, 6, 7, 8 $ are:\n",
       "\n",
       "$$\n",
       "\\boxed{105}, \\boxed{186}, \\boxed{301}, \\boxed{456}\n",
       "$$<|im_end|>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS GENERATED, IMA TEMINATE NOW\n"
     ]
    }
   ],
   "source": [
    "### =======\n",
    "gen_times = []\n",
    "current_step_tokens = []\n",
    "time_per_step = 0\n",
    "### =======\n",
    "\n",
    "# keep a list of generated tokens for printing (including the prefix that is already in cache)\n",
    "writer_output_tokens = tokenizer.encode(prompting.writer_output_prefix, **tokenizer_kwargs).flatten().tolist()\n",
    "thinker_output_tokens = tokenizer.encode(prompting.thinker_output_prefix, **tokenizer_kwargs).flatten().tolist()\n",
    "\n",
    "# write \\n\\n that we have not encoded in cache yet - it will be encoded on the first step for each mode\n",
    "writer_output_tokens.append(tokenizer.encode(\"\\n\\n\", **tokenizer_kwargs).item())\n",
    "thinker_output_tokens.append(tokenizer.encode(\"\\n\\n\", **tokenizer_kwargs).item())\n",
    "\n",
    "cache = AsyncReasoningCache(model, tokenizer, prompting, tokenizer_kwargs=tokenizer_kwargs)\n",
    "with torch.inference_mode():\n",
    "    t0 = time.perf_counter()\n",
    "    for step in range(1024):\n",
    "        if cache.state == State.thinker_only:\n",
    "            next_inputs = {\"input_ids\": torch.tensor([thinker_output_tokens[-1:]], device=device)}\n",
    "            logits = model(**cache.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "            logits[..., forbidden_token_ix] -= 100\n",
    "            thinker_output_tokens.append(int(logits.argmax(-1)))\n",
    "\n",
    "        elif cache.state == State.thinker_and_writer:\n",
    "            next_inputs = {\"input_ids\": torch.tensor([writer_output_tokens[-1:], thinker_output_tokens[-1:]], device=device)}\n",
    "            logits = model(**cache.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "            logits[..., forbidden_token_ix] -= 100\n",
    "            writer_next_token, thinker_next_token = logits.argmax(-1)\n",
    "            writer_output_tokens.append(writer_next_token)\n",
    "            thinker_output_tokens.append(thinker_next_token)\n",
    "\n",
    "            ### =======\n",
    "            t1 = time.perf_counter()\n",
    "            time_per_step += t1 - t0\n",
    "            current_step_tokens.append(writer_next_token.item())\n",
    "            ### =======\n",
    "\n",
    "            if is_end_of_step(writer_output_tokens):  # wait for the thinker's signal to continue\n",
    "                ### =======\n",
    "                gen_times.append((current_step_tokens, tokenizer.decode(current_step_tokens), time_per_step))\n",
    "                current_step_tokens = []\n",
    "                time_per_step = 0\n",
    "                ### =======\n",
    "                cache.state = State.thinker_only\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected state {cache.state}\")\n",
    "\n",
    "        if (step + 1) % 20 == 0 or is_end_of_step(thinker_output_tokens):  # ask thinker if we can continue writing\n",
    "            cache.state = State.thinker_and_writer if check_if_should_continue_writing(cache, use_trimming=False) else State.thinker_only\n",
    "        display_tokens(writer_output_tokens, thinker_output_tokens, cache.state)\n",
    "        if writer_output_tokens[-1] == tokenizer.eos_token_id:\n",
    "            print(\"EOS GENERATED, IMA TEMINATE NOW\")\n",
    "            break\n",
    "\n",
    "### =======\n",
    "os.makedirs(\"evals_dir\", exist_ok=True)\n",
    "with open(\"evals_dir/step_times.json\", \"w\") as f:\n",
    "    json.dump(gen_times, f)\n",
    "### ======="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
