{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=5\n",
      "env: HF_HOME=/mnt/LLM\n",
      "env: OMP_NUM_THREADS=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f573279b7d74a1d87356a3a59363101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=5\n",
    "%env HF_HOME=/mnt/LLM\n",
    "%env OMP_NUM_THREADS=16\n",
    "\n",
    "import sys; sys.path.insert(0, \"../.\");\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from evals.baseline_solver import BaselineSolver as Solver\n",
    "from evals.tts_evaluator import TTSEvaluator\n",
    "from utils.answer_processing import find_last_valid_expression, check_equality_judge, check_equality_local_model\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"  # for 48GB gpus, use \"Qwen/Qwen3-32B-AWQ\" instead\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "## Thinker mode\n",
       "\n",
       "\n",
       "\n",
       "## Writer mode\n",
       "\n",
       "We are asked to calculate the expression:\n",
       "\n",
       "$$\n",
       "x - x^2 + x^3\n",
       "$$\n",
       "\n",
       "for $ values $ x = 5, 6, 7, 8 $. Let's compute each one step by step.\n",
       "\n",
       "---\n",
       "\n",
       "### For $ x = 5 $:\n",
       "\n",
       "$$\n",
       "5 - 5^2 + 5^3 = 5 - 25 + 125 = 105\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "### For $ x = 6 $:\n",
       "$$\n",
       "6 - 6^2 + 6^3 = 6 - 36 + 216 = 186\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "### For $ x = 7 $:\n",
       "$$\n",
       "7 - 7^2 + 7^3 = 7 - 49 + 343 = 291\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "### For $ x = 8 $:\n",
       "$$\n",
       "8 - 8^2 + 8^3 = 8 - 64 + 512 = 456\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "### Final Answers:\n",
       "\n",
       "$$\n",
       "\\boxed{105,\\ 186,\\ 221,\\ 456}\n",
       "$$<|im_end|>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "problem = \"\"\"Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\\\boxed{ }.\"\"\"\n",
    "answer = \"105, 186, 201, 456\"\n",
    "solver = Solver(model, tokenizer, thinker_enabled=False)\n",
    "writer_output_str, thinker_output_str, token_times, eos_generated = solver.solve(problem, budget=1024, display_generation_in_real_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer is correct: False\n"
     ]
    }
   ],
   "source": [
    "response = find_last_valid_expression(writer_output_str, extract_result=lambda x: x[7:-1])\n",
    "use_api_not_local = True\n",
    "if use_api_not_local:\n",
    "    is_equal = check_equality_judge(response, answer)\n",
    "else:\n",
    "    is_equal = check_equality_local_model(model, tokenizer, response, answer)\n",
    "print(f\"Answer is correct: {is_equal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-25 00:01:10,513] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yakushev-ga/Projects/AsyncReasoning/.kernel_venv/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/yakushev-ga/Projects/AsyncReasoning/notebooks/.././tortoise/models/stream_generator.py:141: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/yakushev-ga/Projects/AsyncReasoning/.kernel_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> delay_to_first: 1.427943931426853\n",
      ">> total_delay: 1.8959130305772027\n",
      ">> total_delay_mius1: 0.42794393142685294\n",
      ">> duration_no_delay: 97.16070833333333\n",
      ">> duration_with_delay: 99.05662136391054\n",
      ">> steps_to_first: 0\n",
      ">> delay_steps: 0\n",
      ">> delay_minus10steps: 0\n"
     ]
    }
   ],
   "source": [
    "evaluator = TTSEvaluator()\n",
    "chunks, audio = evaluator.get_chunks_with_tts(token_times, k_chunks=5, return_audio=True)\n",
    "metrics = evaluator(**chunks, add_tts_in_parrallel=True, return_delays=False)\n",
    "\n",
    "indent = 2\n",
    "for k, v in metrics.items():\n",
    "    pad = \">\" * indent\n",
    "    if isinstance(v, dict):\n",
    "        print(f\"{pad} {k}:\")\n",
    "        pretty_dict(v, indent + 4)\n",
    "    else:\n",
    "        print(f\"{pad} {k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".kernel_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
