{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=5\n",
      "env: HF_HOME=/mnt/LLM\n",
      "env: OMP_NUM_THREADS=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e1c482a4414e648836e37005238205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=5\n",
    "%env HF_HOME=/mnt/LLM\n",
    "%env OMP_NUM_THREADS=16\n",
    "\n",
    "import sys; sys.path.insert(0, \"../.\");\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from async_reasoning.solver import AsyncReasoningSolver as Solver\n",
    "from evals.tts_evaluator import TTSEvaluator\n",
    "from utils.answer_processing import find_last_valid_expression, check_equality_judge, check_equality_local_model\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"  # for 48GB gpus, use \"Qwen/Qwen3-32B-AWQ\" instead\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)\n",
    "\n",
    "system_tokens = [key for key in tokenizer.vocab.keys() if key.endswith(\"SYSTEM\") or key.endswith(\"SYSTEM:\")]\n",
    "writer_forbidden_token_ix = [tokenizer.vocab[x] for x in [\"</think>\", \"<|im_start|>\", \"<|endoftext|>\"] + system_tokens]\n",
    "thinker_forbidden_token_ix = [tokenizer.vocab[x] for x in [\"</think>\", \"<|im_start|>\", \"<|im_end|>\", \"<|endoftext|>\"] + system_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# State.thinker_and_writer\n",
       "\n",
       "## Thinker mode\n",
       "\n",
       "\n",
       "<think>\n",
       "I am in Thinker mode. My text is not visible to the user. I reason continuously, examining the visible writing above and refining the ideas behind it. I detect errors, test assumptions, and plan improvements. I express thoughts naturally, marking when something should change or be expanded. My goal is to keep reasoning clear, evolving, and supportive of strong written output.\n",
       "\n",
       "Okay, let's see. The user wants me to calculate the expression x - xÂ² + xÂ³ for x values 5, 6, 7, and 8. I need to compute each one step by step.\n",
       "\n",
       "Starting with x = 5. The expression is 5 - 5Â² + 5Â³. Let's break it down: 5 squared is 25, and 5 cubed is 125. So substituting those in, it becomes 5 - 25 + 125. Calculating that: 5 - 25 is -20, and then adding 125 gives 105. So for x=5, the result is 105.\n",
       "\n",
       "Next, x = 6. The expression is 6 - 6Â² + 6Â³. 6 squared is 36, and 6 cubed is 216. Substituting those in: 6 - 36 + 216. 6 - 36 is -30, and adding 216 gives 186. So for x=6, the result is 186.\n",
       "\n",
       "Now x = 7. The expression is 7 - 7Â² + 7Â³. 7 squared is 49, and 7 cubed is 343. Substituting: 7 - 49 + 343. 7 - 49 is -42, and adding 343 gives 301. So for x=7, the result is 301.\n",
       "\n",
       "Finally, x = 8. The expression is 8 - 8Â² + 8Â³. 8 squared is 64, and 8 cubed is 512. Substituting: 8 - 64 + 512. 8 - 64 is -56, and adding 512 gives 456. So for x=8, the result is 456.\n",
       "\n",
       "Let me double-check each calculation to make sure I didn\n",
       "\n",
       "## Writer mode\n",
       "\n",
       "\n",
       "I am in Writer mode. My text is visible to the user. I focus on clear, precise expression and careful word choice. I write only what is well-reasoned and verified in my workspace. I never speculate or improvise. If my thinking shifts or reveals an error, I immediately adjust. My goal is calm, accurate, and readable output.\n",
       "\n",
       "We are asked to evaluate the expression $ x - x^2 + x^3 $ for $ x = 5, 6, 7, 8 $. Let's compute each value step by step.\n",
       "\n",
       "---\n",
       "\n",
       "**For $ x = 5 $:**\n",
       "\n",
       "$$\n",
       "5 - 5^2 + 5^3 = 5 - 25 + 125 = 105\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "**For $ x = 6 $:**\n",
       "\n",
       "$$\n",
       "6 - 6^2 + 6^3 = 6 - 36 + 216 = 186\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "**For $ x = 7 $:**\n",
       "\n",
       "$$\n",
       "7 - 7^2 + 7^3 = 7 - 49 + 343 = 201\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "**For $ x = 8 $:**\n",
       "\n",
       "$$\n",
       "8 - 8^2 + 8^3 = 8 - 64 + 512 = 456\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "The results are:\n",
       "\n",
       "$$\n",
       "\\boxed{105, 186, 201, 456}\n",
       "$$<|im_end|>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "problem = \"\"\"Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\\\boxed{ }.\"\"\"\n",
    "answer = \"105, 186, 201, 456\"\n",
    "solver = Solver(model, \n",
    "                tokenizer, \n",
    "                writer_forbidden_token_ix=writer_forbidden_token_ix, \n",
    "                thinker_forbidden_token_ix=thinker_forbidden_token_ix,\n",
    "                use_fast_kernel=False)\n",
    "writer_output_str, thinker_output_str, token_times, eos_generated = solver.solve(problem, budget=1024, display_generation_in_real_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer is correct: True\n"
     ]
    }
   ],
   "source": [
    "response = find_last_valid_expression(writer_output_str, extract_result=lambda x: x[7:-1])\n",
    "use_api_not_local = True\n",
    "if use_api_not_local:\n",
    "    is_equal = check_equality_judge(response, answer)\n",
    "else:\n",
    "    is_equal = check_equality_local_model(model, tokenizer, response, answer)\n",
    "print(f\"Answer is correct: {is_equal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-21 01:46:27,635] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yakushev-ga/Projects/AsyncReasoning/.kernel_venv/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/yakushev-ga/Projects/AsyncReasoning/notebooks/.././tortoise/models/stream_generator.py:141: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/yakushev-ga/Projects/AsyncReasoning/.kernel_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Free memory : 15.562378 (GigaBytes)  \n",
      "Total memory: 79.250732 (GigaBytes)  \n",
      "Requested memory: 0.167969 (GigaBytes) \n",
      "Setting maximum total tokens (input + output) to 1024 \n",
      "WorkSpace: 0x7f43f4000000 \n",
      "------------------------------------------------------\n",
      ">> delay_to_first: 2.97106787096709\n",
      ">> total_delay: 2.97106787096709\n",
      ">> total_delay_mius1: 1.9710678709670901\n",
      ">> duration_no_delay: 65.45004166666666\n",
      ">> duration_with_delay: 68.42110953763375\n",
      ">> steps_to_first: 24\n",
      ">> delay_steps: 196\n",
      ">> delay_minus10steps: 76\n"
     ]
    }
   ],
   "source": [
    "evaluator = TTSEvaluator()\n",
    "chunks, audio = evaluator.get_chunks_with_tts(token_times, k_chunks=5, return_audio=True)\n",
    "metrics = evaluator(**chunks, add_tts_in_parrallel=True, return_delays=False)\n",
    "\n",
    "indent = 2\n",
    "for k, v in metrics.items():\n",
    "    pad = \">\" * indent\n",
    "    if isinstance(v, dict):\n",
    "        print(f\"{pad} {k}:\")\n",
    "        pretty_dict(v, indent + 4)\n",
    "    else:\n",
    "        print(f\"{pad} {k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "async",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
